{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6355debc-f38e-4919-850f-e7c5190b9991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:23:42.588683Z",
     "iopub.status.busy": "2025-08-31T13:23:42.588414Z",
     "iopub.status.idle": "2025-08-31T13:28:27.234550Z",
     "shell.execute_reply": "2025-08-31T13:28:27.233991Z",
     "shell.execute_reply.started": "2025-08-31T13:23:42.588664Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-31 13:23:50 [__init__.py:244] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.7.5: Fast Llama patching. Transformers: 4.53.3. vLLM: 0.9.2.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 21.975 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit with actual GPU utilization = 69.08%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 21.98 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 224.\n",
      "Unsloth: vLLM's KV Cache can use up to 9.0 GB. Also swap space = 6 GB.\n",
      "INFO 08-31 13:24:03 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 08-31 13:24:03 [config.py:1472] Using max model len 2048\n",
      "INFO 08-31 13:24:03 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 08-31 13:24:04 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 13:24:04,540 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 13:24:05 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-31 13:24:05 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 08-31 13:24:05 [gpu_model_runner.py:1770] Starting to load model unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit...\n",
      "INFO 08-31 13:24:05 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 08-31 13:24:05 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
      "INFO 08-31 13:24:05 [bitsandbytes_loader.py:499] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 08-31 13:24:05 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684902d1b1414fa6a7e6ea9e7d864523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 13:24:11 [weight_utils.py:308] Time spent downloading weights for unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit: 5.916271 seconds\n",
      "INFO 08-31 13:24:11 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187b0cf1854b4631bc2c6fa4f0fab1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ce3b39e72b424a97bcd749b2f7b86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 13:24:12 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 08-31 13:24:13 [gpu_model_runner.py:1801] Model loading took 5.7737 GiB and 7.571209 seconds\n",
      "INFO 08-31 13:24:25 [backends.py:508] Using cache directory: /home/sagemaker-user/.cache/vllm/torch_compile_cache/7b30a9cbc9/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-31 13:24:25 [backends.py:519] Dynamo bytecode transform time: 11.41 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:01<00:00,  6.91it/s, triton_poi_fused_view_6]                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 13:24:28 [backends.py:181] Cache the graph of shape None for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.99it/s, triton_poi_fused_view_8]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 74.38it/s, triton_poi_fused_view_8]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 58.00it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 61.42it/s, triton_poi_fused_view_8]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 162.71it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 57.99it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 176.97it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 174.40it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 176.35it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 52.32it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 189.99it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 185.98it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 206.69it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 192.96it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 192.59it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.59it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 184.45it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 203.02it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 177.97it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 179.09it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 42.24it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 202.75it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 185.42it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 204.93it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.76it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 192.95it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 191.07it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 198.37it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 183.99it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 172.74it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 191.08it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 14.04it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 13:25:08 [backends.py:193] Compiling a graph for general shape takes 42.14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 13:26:19 [monitor.py:34] torch.compile takes 53.55 s in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 13:26:21,336 - INFO - flashinfer.jit: Loading JIT ops: sampling\n",
      "2025-08-31 13:27:00,586 - INFO - flashinfer.jit: Finished loading JIT ops: sampling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 13:27:01 [gpu_worker.py:232] Available KV cache memory: 8.93 GiB\n",
      "INFO 08-31 13:27:02 [kv_cache_utils.py:716] GPU KV cache size: 73,184 tokens\n",
      "INFO 08-31 13:27:02 [kv_cache_utils.py:720] Maximum concurrency for 2,048 tokens per request: 35.73x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [01:11<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 13:28:13 [gpu_model_runner.py:2326] Graph capturing finished in 72 secs, took 1.44 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 13:28:14 [core.py:172] init engine (profile, create kv cache, warmup model) took 240.70 seconds\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'q_norm', 'pre_feedforward_layernorm', 'k_norm']\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'q_norm', 'pre_feedforward_layernorm', 'k_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    # load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.70\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ], # Remove QKVO if out of memory\n",
    "    lora_alpha = lora_rank,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", # Enable long context finetuning\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilised LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f68dad8-a1e6-4e87-9ed6-e23ac2123f74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:35:05.577531Z",
     "iopub.status.busy": "2025-08-31T13:35:05.577060Z",
     "iopub.status.idle": "2025-08-31T13:35:05.580408Z",
     "shell.execute_reply": "2025-08-31T13:35:05.579741Z",
     "shell.execute_reply.started": "2025-08-31T13:35:05.577513Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8844ff6831c9254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T20:51:12.760191Z",
     "start_time": "2025-07-08T20:51:12.748278Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-31T13:40:16.055149Z",
     "iopub.status.busy": "2025-08-31T13:40:16.054878Z",
     "iopub.status.idle": "2025-08-31T13:40:16.393481Z",
     "shell.execute_reply": "2025-08-31T13:40:16.392885Z",
     "shell.execute_reply.started": "2025-08-31T13:40:16.055133Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "pd_df = pd.read_csv('hotel_2000.csv', sep=',')\n",
    "pd_df\n",
    "\n",
    "# Only use the below code if you haven't already split your datasets\n",
    "unique_convs = pd_df['conv_id'].unique().tolist()\n",
    "random.shuffle(unique_convs)  # shuffle to randomize split\n",
    "\n",
    "split_idx = int(len(unique_convs) * 0.5)  # 50% train, 50% test\n",
    "sft_convs = set(unique_convs[:split_idx])\n",
    "grpo_convs = set(unique_convs[split_idx:])\n",
    "\n",
    "sft_train = pd_df[pd_df['conv_id'].isin(sft_convs)].reset_index(drop=True)\n",
    "grpo_train = pd_df[pd_df['conv_id'].isin(grpo_convs)].reset_index(drop=True)\n",
    "\n",
    "# sft_train.to_csv('hotel_sft_train2000.csv', index=False, sep=',')\n",
    "# grpo_train.to_csv('hotel_grpo_train2000.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c762cc0f-0794-45d5-a29c-2c7a0cede4d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:40:20.814638Z",
     "iopub.status.busy": "2025-08-31T13:40:20.814112Z",
     "iopub.status.idle": "2025-08-31T13:40:21.502469Z",
     "shell.execute_reply": "2025-08-31T13:40:21.501966Z",
     "shell.execute_reply.started": "2025-08-31T13:40:20.814622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sft_total': 1655, 'sft_partA': 826, 'sft_partB': 829, 'grpo_total': 1680, 'grpo_partA': 855, 'grpo_partB': 825}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def split_by_conv(df, frac=0.5, id_col=\"conv_id\", seed=42):\n",
    "    # Split on unique conversation ids to avoid leakage\n",
    "    rnd = random.Random(seed)\n",
    "    unique_ids = df[id_col].unique().tolist()\n",
    "    rnd.shuffle(unique_ids)\n",
    "    cut = int(len(unique_ids) * frac)\n",
    "    a_ids = set(unique_ids[:cut])\n",
    "    b_ids = set(unique_ids[cut:])\n",
    "    part_a = df[df[id_col].isin(a_ids)].reset_index(drop=True)\n",
    "    part_b = df[df[id_col].isin(b_ids)].reset_index(drop=True)\n",
    "    return part_a, part_b\n",
    "\n",
    "# Create ~50/50 splits for sft_train\n",
    "sft_a, sft_b = split_by_conv(sft_train, frac=0.5, id_col=\"conv_id\", seed=42)\n",
    "# Create ~50/50 splits for grpo_train\n",
    "grpo_a, grpo_b = split_by_conv(grpo_train, frac=0.5, id_col=\"conv_id\", seed=42)\n",
    "\n",
    "# Optional: save to CSVs\n",
    "sft_a.to_csv(\"hotel_sft_train_partA_500.csv\", index=False)\n",
    "sft_b.to_csv(\"hotel_sft_train_partB_500.csv\", index=False)\n",
    "grpo_a.to_csv(\"hotel_grpo_train_partA_500.csv\", index=False)\n",
    "grpo_b.to_csv(\"hotel_grpo_train_partB_500.csv\", index=False)\n",
    "\n",
    "# Quick sanity check sizes\n",
    "print({\n",
    "    \"sft_total\": len(sft_train),\n",
    "    \"sft_partA\": len(sft_a),\n",
    "    \"sft_partB\": len(sft_b),\n",
    "    \"grpo_total\": len(grpo_train),\n",
    "    \"grpo_partA\": len(grpo_a),\n",
    "    \"grpo_partB\": len(grpo_b),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84092e34-3328-4905-bc4c-e76eddddccf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:40:40.640905Z",
     "iopub.status.busy": "2025-08-31T13:40:40.640552Z",
     "iopub.status.idle": "2025-08-31T13:40:40.644903Z",
     "shell.execute_reply": "2025-08-31T13:40:40.644289Z",
     "shell.execute_reply.started": "2025-08-31T13:40:40.640889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0e86f75-56dc-4cd2-b8f0-7ee62ef76885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:40:41.825329Z",
     "iopub.status.busy": "2025-08-31T13:40:41.824959Z",
     "iopub.status.idle": "2025-08-31T13:40:41.828778Z",
     "shell.execute_reply": "2025-08-31T13:40:41.828252Z",
     "shell.execute_reply.started": "2025-08-31T13:40:41.825312Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_for_sft(sample):\n",
    "    sample[\"input\"] = sample[\"input\"].replace('<|eot_id|>', \"\")\n",
    "    prompt = f'{sample[\"input\"]}{sample[\"ground_truth\"]}{tokenizer.eos_token}'\n",
    "    sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "    sample[\"query\"] = tokenizer.decode(sample['input_ids'])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ae37515-0b0b-4b5e-9b9f-9def84b1606b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:05.207774Z",
     "iopub.status.busy": "2025-08-31T13:41:05.207383Z",
     "iopub.status.idle": "2025-08-31T13:41:05.210529Z",
     "shell.execute_reply": "2025-08-31T13:41:05.209983Z",
     "shell.execute_reply.started": "2025-08-31T13:41:05.207757Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_to_use = sft_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa9a1ccbdc084e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T21:00:55.813449Z",
     "start_time": "2025-07-08T21:00:55.151806Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:11.770851Z",
     "iopub.status.busy": "2025-08-31T13:41:11.770518Z",
     "iopub.status.idle": "2025-08-31T13:41:11.800800Z",
     "shell.execute_reply": "2025-08-31T13:41:11.800282Z",
     "shell.execute_reply.started": "2025-08-31T13:41:11.770835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conv_id', 'user_utterance', 'system_utterance', 'input', 'output', 'ground_truth'],\n",
       "        num_rows: 826\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "dataset_dict = {}\n",
    "dataset_dict['train'] = datasets.Dataset.from_pandas(pd_to_use)\n",
    "training_ddt = datasets.DatasetDict(dataset_dict)\n",
    "training_ddt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfbd4b5e-da09-4415-95ff-da800f4c8789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:14.666904Z",
     "iopub.status.busy": "2025-08-31T13:41:14.666479Z",
     "iopub.status.idle": "2025-08-31T13:41:17.873808Z",
     "shell.execute_reply": "2025-08-31T13:41:17.873213Z",
     "shell.execute_reply.started": "2025-08-31T13:41:14.666887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f891b6f4d7045e4add06e389002189d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conv_id', 'user_utterance', 'system_utterance', 'input', 'output', 'ground_truth', 'input_ids', 'query'],\n",
       "        num_rows: 826\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ddt = training_ddt.map(tokenize_for_sft, batched=False)\n",
    "training_ddt.set_format(type=\"torch\")\n",
    "training_ddt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a1d2056-1ed3-4548-a421-bb9960f51811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:18.603988Z",
     "iopub.status.busy": "2025-08-31T13:41:18.603635Z",
     "iopub.status.idle": "2025-08-31T13:41:18.608409Z",
     "shell.execute_reply": "2025-08-31T13:41:18.607877Z",
     "shell.execute_reply.started": "2025-08-31T13:41:18.603971Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a conversational agent with the following persona:\n",
      "You are a helpful hotel assistant, your job is to help users in whatever queries they may have.\n",
      "\n",
      "ALLOWED INTENTS:\n",
      "{'book_room': 'The user wants to book a room in the hotel', 'cancel_booking': 'The user wants to cancel an existing booking', 'general_enquiries': 'The user wants to ask general questions about the hotel', 'chit_chat': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be 'Sorry, I can only help you with hotel queries.'\"}\n",
      "\n",
      "ALLOWED SLOTS (must match exactly):\n",
      "{'book_to', 'book_room', 'bookingID', 'dateTo', 'cancel_booking', 'dateFrom'}\n",
      "\n",
      "ALLOWED ACTIONS (with their required slots):\n",
      "{'makeBooking': ('dateFrom', 'dateTo'), 'lookUpBooking': 'bookingID', 'cancellation': 'bookingID'}\n",
      "\n",
      "TASK:\n",
      "Given the current conversation history, generate EXACTLY one JSON object that describes ONLY the next system turn. The output MUST be a single JSON object and nothing else.\n",
      "\n",
      "REQUIRED JSON SCHEMA (top-level keys and order MUST be exactly):\n",
      "    \"system_response\" : string\n",
      "    \"dialogue_acts\"   : object with keys {{ \"intent\": string, \"action\": string}} â€” \"action\" may be \"\" if none\n",
      "    \"belief_state\"    : object with ALL slots from {'book_to', 'book_room', 'bookingID', 'dateTo', 'cancel_booking', 'dateFrom'} as keys (ONLY include filled slots here)\n",
      "\n",
      "STRICT FORMAT RULES:\n",
      "1. Output must be ONLY the JSON object â€” no extra text, no explanations, no labels, no prefixes, no suffixes, no additional JSON objects.\n",
      "2. Do not add or remove keys. Do not reorder top-level keys.\n",
      "3. Use ONLY intents from {'book_room': 'The user wants to book a room in the hotel', 'cancel_booking': 'The user wants to cancel an existing booking', 'general_enquiries': 'The user wants to ask general questions about the hotel', 'chit_chat': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be 'Sorry, I can only help you with hotel queries.'\"}, slots from {'book_to', 'book_room', 'bookingID', 'dateTo', 'cancel_booking', 'dateFrom'}, and actions from {'makeBooking': ('dateFrom', 'dateTo'), 'lookUpBooking': 'bookingID', 'cancellation': 'bookingID'}. Do not invent or abbreviate any names. Strings must match exactly.\n",
      "4. If the SYSTEM turn includes a slot reference AND an action, replace slot values in the \"system_response\" with the placeholder format \"<slot_name>\".\n",
      "5. The \"belief_state\" object must contain ONLY allowed slots which are filled.\n",
      "6. \"system_response\" must not repeat the user's last utterance verbatim. It should advance the conversation.\n",
      "7. Wording should be natural, concise, and free of extraneous commentary.\n",
      "8. You are ONLY to generate 1 JSON object as your response. Do not generate duplicate entries.\n",
      "\n",
      "DIVERSITY RULES:\n",
      "1. Vary phrasings used in your \"system_response\" compared to earlier turns in the same or previous conversations.\n",
      "2. Avoid reusing the same wording or slot combinations already seen in prior outputs for the same intent.\n",
      "\n",
      "EXAMPLES:\n",
      "(keep examples out of final output; they are for guidance only)\n",
      "Example 1:\n",
      "CONV_HISTORY:\n",
      "USER: I want to book a hotel room from August 12th to August 15th.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Got it. I'll reserve a room for you from August 12th to August 15th. Do you have a preferred bed type?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"book_room\", \"action\" : \"makeBooking\"}},\n",
      "    \"belief_state\": {{\"dateFrom\" : \"August 12th\", \"dateTo\" : \"August 15th\"}}\n",
      "}}\n",
      "\n",
      "Example 2:\n",
      "CONV_HISTORY:\n",
      "SYSTEM: How can I help you today?\n",
      "USER: I'd like to cancel my hotel booking please.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Sure, I can cancel your booking. Could you provide your booking ID?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"cancel_booking\", \"action\" : \"\"}},\n",
      "    \"belief_state\": {{}}\n",
      "}}\n",
      "\n",
      "Example 3\n",
      "CONV_HISTORY:\n",
      "USER: My booking ID is 78910.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Thanks. I've cancelled your reservation with booking ID 78910. Is there anything else I can help you with?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"cancel_booking\", \"action\" : \"cancelBooking\"}},\n",
      "    \"belief_state\": {{\"bookingID\" : \"78910\"}}\n",
      "}}\n",
      "\n",
      "Example 4:\n",
      "CONV_HISTORY:\n",
      "USER: Do you have a suite available for this weekend?\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Let me check. Could you tell me your exact check-in and check-out dates?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"book_room\", \"action\" : \"\"}},\n",
      "    \"belief_state\": {{}}\n",
      "}}\n",
      "\n",
      "Example 5:\n",
      "CONV_HISTORY:\n",
      "USER: I want to change my check-out date to July 22nd.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"No problem, I'll update your check-out date to July 22nd. Is the check-in date staying the same?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"modify_booking\", \"action\" : \"updateBooking\"}},\n",
      "    \"belief_state\": {{\"dateFrom\" : \"July 15th \", \"dateTo\" : \"July 22nd\", \"bookingID\" : \"BK55667\"}}\n",
      "}}\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Conversation history so far:\n",
      "USER: USER: I need a room for my pet rock collection. They get lonely without me.\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Dialogue State:\n",
      "{'system_response': \"SYSTEM: I see. While we don't have specific accommodations for pet rocks, I can certainly help you book a room. When would you like to check in and out?\", 'dialogue_acts': {'intent': 'book_room', 'action': ''}, 'belief_state': {}}<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(training_ddt['train'][0]['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f36f2d-161f-4012-be49-fcb875876f81",
   "metadata": {},
   "source": [
    "# SFT FIRST!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fb37631-daa5-4c2b-9a9d-b1a218aa66be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:19.725669Z",
     "iopub.status.busy": "2025-08-31T13:41:19.725349Z",
     "iopub.status.idle": "2025-08-31T13:41:19.755896Z",
     "shell.execute_reply": "2025-08-31T13:41:19.755377Z",
     "shell.execute_reply.started": "2025-08-31T13:41:19.725654Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a conversational agent with the following persona:\n",
      "You are a helpful hotel assistant, your job is to help users in whatever queries they may have.\n",
      "\n",
      "ALLOWED INTENTS:\n",
      "{'book_room': 'The user wants to book a room in the hotel', 'cancel_booking': 'The user wants to cancel an existing booking', 'general_enquiries': 'The user wants to ask general questions about the hotel', 'chit_chat': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be 'Sorry, I can only help you with hotel queries.'\"}\n",
      "\n",
      "ALLOWED SLOTS (must match exactly):\n",
      "{'book_to', 'book_room', 'bookingID', 'dateTo', 'cancel_booking', 'dateFrom'}\n",
      "\n",
      "ALLOWED ACTIONS (with their required slots):\n",
      "{'makeBooking': ('dateFrom', 'dateTo'), 'lookUpBooking': 'bookingID', 'cancellation': 'bookingID'}\n",
      "\n",
      "TASK:\n",
      "Given the current conversation history, generate EXACTLY one JSON object that describes ONLY the next system turn. The output MUST be a single JSON object and nothing else.\n",
      "\n",
      "REQUIRED JSON SCHEMA (top-level keys and order MUST be exactly):\n",
      "    \"system_response\" : string\n",
      "    \"dialogue_acts\"   : object with keys {{ \"intent\": string, \"action\": string}} â€” \"action\" may be \"\" if none\n",
      "    \"belief_state\"    : object with ALL slots from {'book_to', 'book_room', 'bookingID', 'dateTo', 'cancel_booking', 'dateFrom'} as keys (ONLY include filled slots here)\n",
      "\n",
      "STRICT FORMAT RULES:\n",
      "1. Output must be ONLY the JSON object â€” no extra text, no explanations, no labels, no prefixes, no suffixes, no additional JSON objects.\n",
      "2. Do not add or remove keys. Do not reorder top-level keys.\n",
      "3. Use ONLY intents from {'book_room': 'The user wants to book a room in the hotel', 'cancel_booking': 'The user wants to cancel an existing booking', 'general_enquiries': 'The user wants to ask general questions about the hotel', 'chit_chat': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be 'Sorry, I can only help you with hotel queries.'\"}, slots from {'book_to', 'book_room', 'bookingID', 'dateTo', 'cancel_booking', 'dateFrom'}, and actions from {'makeBooking': ('dateFrom', 'dateTo'), 'lookUpBooking': 'bookingID', 'cancellation': 'bookingID'}. Do not invent or abbreviate any names. Strings must match exactly.\n",
      "4. If the SYSTEM turn includes a slot reference AND an action, replace slot values in the \"system_response\" with the placeholder format \"<slot_name>\".\n",
      "5. The \"belief_state\" object must contain ONLY allowed slots which are filled.\n",
      "6. \"system_response\" must not repeat the user's last utterance verbatim. It should advance the conversation.\n",
      "7. Wording should be natural, concise, and free of extraneous commentary.\n",
      "8. You are ONLY to generate 1 JSON object as your response. Do not generate duplicate entries.\n",
      "\n",
      "DIVERSITY RULES:\n",
      "1. Vary phrasings used in your \"system_response\" compared to earlier turns in the same or previous conversations.\n",
      "2. Avoid reusing the same wording or slot combinations already seen in prior outputs for the same intent.\n",
      "\n",
      "EXAMPLES:\n",
      "(keep examples out of final output; they are for guidance only)\n",
      "Example 1:\n",
      "CONV_HISTORY:\n",
      "USER: I want to book a hotel room from August 12th to August 15th.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Got it. I'll reserve a room for you from August 12th to August 15th. Do you have a preferred bed type?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"book_room\", \"action\" : \"makeBooking\"}},\n",
      "    \"belief_state\": {{\"dateFrom\" : \"August 12th\", \"dateTo\" : \"August 15th\"}}\n",
      "}}\n",
      "\n",
      "Example 2:\n",
      "CONV_HISTORY:\n",
      "SYSTEM: How can I help you today?\n",
      "USER: I'd like to cancel my hotel booking please.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Sure, I can cancel your booking. Could you provide your booking ID?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"cancel_booking\", \"action\" : \"\"}},\n",
      "    \"belief_state\": {{}}\n",
      "}}\n",
      "\n",
      "Example 3\n",
      "CONV_HISTORY:\n",
      "USER: My booking ID is 78910.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Thanks. I've cancelled your reservation with booking ID 78910. Is there anything else I can help you with?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"cancel_booking\", \"action\" : \"cancelBooking\"}},\n",
      "    \"belief_state\": {{\"bookingID\" : \"78910\"}}\n",
      "}}\n",
      "\n",
      "Example 4:\n",
      "CONV_HISTORY:\n",
      "USER: Do you have a suite available for this weekend?\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Let me check. Could you tell me your exact check-in and check-out dates?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"book_room\", \"action\" : \"\"}},\n",
      "    \"belief_state\": {{}}\n",
      "}}\n",
      "\n",
      "Example 5:\n",
      "CONV_HISTORY:\n",
      "USER: I want to change my check-out date to July 22nd.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"No problem, I'll update your check-out date to July 22nd. Is the check-in date staying the same?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"modify_booking\", \"action\" : \"updateBooking\"}},\n",
      "    \"belief_state\": {{\"dateFrom\" : \"July 15th \", \"dateTo\" : \"July 22nd\", \"bookingID\" : \"BK55667\"}}\n",
      "}}\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Conversation history so far:\n",
      "USER: USER: I need a room for my pet rock collection. They get lonely without me.\n",
      "SYSTEM: SYSTEM: I see. While we don't have specific accommodations for pet rocks, I can certainly help you book a room. When would you like to check in and out?\n",
      "USER: USER: From the next blue moon to the following harvest moon, please.\n",
      "SYSTEM: SYSTEM: I apologize, but I need specific dates for your booking. Could you please provide the check-in and check-out dates in a standard format?\n",
      "USER: USER: Oh, right! How about July 15th to July 20th? My rocks prefer summer.\n",
      "SYSTEM: SYSTEM: Thank you for providing the dates. I can book a room for you from <dateFrom> to <dateTo>. Is there anything specific you need in the room for your rock collection?\n",
      "USER: USER: Yes! I need a room with good sunlight. Rocks need their vitamin D too!\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Dialogue State:\n",
      "{'system_response': \"SYSTEM: I understand. While I can't guarantee specific sunlight conditions, I can request a room with a window. I've noted your preference for a bright room. Is there anything else you need?\", 'dialogue_acts': {'intent': 'book_room', 'action': ''}, 'belief_state': {'dateFrom': 'July 15th', 'dateTo': 'July 20th'}}<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.Dataset.from_dict({'text': training_ddt['train']['query']})\n",
    "print(training_data[3]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ca31319-ff73-408f-ac8b-009a916ec058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:20.696366Z",
     "iopub.status.busy": "2025-08-31T13:41:20.695963Z",
     "iopub.status.idle": "2025-08-31T13:41:20.698717Z",
     "shell.execute_reply": "2025-08-31T13:41:20.698259Z",
     "shell.execute_reply.started": "2025-08-31T13:41:20.696350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Validation doesn't work with unsloth training :(\n",
    "\n",
    "# split_dataset = training_data.train_test_split(test_size=0.15, seed=42)\n",
    "\n",
    "# training_data = split_dataset[\"train\"]\n",
    "# validation_data = split_dataset[\"test\"]\n",
    "\n",
    "\n",
    "# print(f\"Training data size: {len(training_data)}\")\n",
    "# print(f\"Validation data size: {len(validation_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b568d60-b7a5-4536-817e-e4205871ed0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:24.722787Z",
     "iopub.status.busy": "2025-08-31T13:41:24.722430Z",
     "iopub.status.idle": "2025-08-31T13:41:24.739710Z",
     "shell.execute_reply": "2025-08-31T13:41:24.739200Z",
     "shell.execute_reply.started": "2025-08-31T13:41:24.722771Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/31 13:41:24 INFO mlflow.tracking.fluent: Experiment with name 'llama3_sft_experiment_hotel_500' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/sagemaker-user/mlruns/190038803992163367', creation_time=1756647684736, experiment_id='190038803992163367', last_update_time=1756647684736, lifecycle_stage='active', name='llama3_sft_experiment_hotel_500', tags={}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "mlflow.set_experiment(\"llama3_sft_experiment_hotel_500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24b5e37c-c5c0-4d58-b54c-314fa952aae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:26.628913Z",
     "iopub.status.busy": "2025-08-31T13:41:26.628445Z",
     "iopub.status.idle": "2025-08-31T13:41:26.631968Z",
     "shell.execute_reply": "2025-08-31T13:41:26.631470Z",
     "shell.execute_reply.started": "2025-08-31T13:41:26.628879Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71729b0b-dfc1-4ce8-acb4-3a26508c2ca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:39.439946Z",
     "iopub.status.busy": "2025-08-31T13:41:39.439347Z",
     "iopub.status.idle": "2025-08-31T13:41:42.607884Z",
     "shell.execute_reply": "2025-08-31T13:41:42.607285Z",
     "shell.execute_reply.started": "2025-08-31T13:41:39.439930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdb42ac3cb64c20a80ae7246a48acb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = training_data,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 10,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run. Use num_train_epochs for full dataset passes.\n",
    "        # max_steps = 80, # Use max_steps if you want exactly 60 updates regardless of dataset size.\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        learning_rate = 2e-4, # or 5e-5 if dataset is bigger than 500 examples\n",
    "        logging_steps = 1, # If using num_train_epochs, you may want to set this at a higher value\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to=\"mlflow\",  # ðŸ‘ˆ Enable MLflow logging\n",
    "        dataloader_num_workers=4,  # Parallel data loading\n",
    "        dataloader_pin_memory=True,  # Speed up data transfer to GPU\n",
    "        dataloader_persistent_workers=True, # Reuse dataloader workers. Important for reducing I/O time.ï¿½\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cc16270-651a-49e6-8ef3-d4a8576002a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:44.174368Z",
     "iopub.status.busy": "2025-08-31T13:41:44.173940Z",
     "iopub.status.idle": "2025-08-31T13:41:44.178398Z",
     "shell.execute_reply": "2025-08-31T13:41:44.177918Z",
     "shell.execute_reply.started": "2025-08-31T13:41:44.174350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A10G. Max memory = 21.975 GB.\n",
      "15.447 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76a5681d-9caf-4777-8c3e-0b2490d56d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:44.940490Z",
     "iopub.status.busy": "2025-08-31T13:41:44.940192Z",
     "iopub.status.idle": "2025-08-31T13:41:44.945262Z",
     "shell.execute_reply": "2025-08-31T13:41:44.944790Z",
     "shell.execute_reply.started": "2025-08-31T13:41:44.940474Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a conversational agent with the following persona:\\nYou are a helpful hotel assistant, your job is to help users in whatever queries they may have.\\n\\nALLOWED INTENTS:\\n{\\'book_room\\': \\'The user wants to book a room in the hotel\\', \\'cancel_booking\\': \\'The user wants to cancel an existing booking\\', \\'general_enquiries\\': \\'The user wants to ask general questions about the hotel\\', \\'chit_chat\\': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be \\'Sorry, I can only help you with hotel queries.\\'\"}\\n\\nALLOWED SLOTS (must match exactly):\\n{\\'book_to\\', \\'book_room\\', \\'bookingID\\', \\'dateTo\\', \\'cancel_booking\\', \\'dateFrom\\'}\\n\\nALLOWED ACTIONS (with their required slots):\\n{\\'makeBooking\\': (\\'dateFrom\\', \\'dateTo\\'), \\'lookUpBooking\\': \\'bookingID\\', \\'cancellation\\': \\'bookingID\\'}\\n\\nTASK:\\nGiven the current conversation history, generate EXACTLY one JSON object that describes ONLY the next system turn. The output MUST be a single JSON object and nothing else.\\n\\nREQUIRED JSON SCHEMA (top-level keys and order MUST be exactly):\\n    \"system_response\" : string\\n    \"dialogue_acts\"   : object with keys {{ \"intent\": string, \"action\": string}} â€” \"action\" may be \"\" if none\\n    \"belief_state\"    : object with ALL slots from {\\'book_to\\', \\'book_room\\', \\'bookingID\\', \\'dateTo\\', \\'cancel_booking\\', \\'dateFrom\\'} as keys (ONLY include filled slots here)\\n\\nSTRICT FORMAT RULES:\\n1. Output must be ONLY the JSON object â€” no extra text, no explanations, no labels, no prefixes, no suffixes, no additional JSON objects.\\n2. Do not add or remove keys. Do not reorder top-level keys.\\n3. Use ONLY intents from {\\'book_room\\': \\'The user wants to book a room in the hotel\\', \\'cancel_booking\\': \\'The user wants to cancel an existing booking\\', \\'general_enquiries\\': \\'The user wants to ask general questions about the hotel\\', \\'chit_chat\\': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be \\'Sorry, I can only help you with hotel queries.\\'\"}, slots from {\\'book_to\\', \\'book_room\\', \\'bookingID\\', \\'dateTo\\', \\'cancel_booking\\', \\'dateFrom\\'}, and actions from {\\'makeBooking\\': (\\'dateFrom\\', \\'dateTo\\'), \\'lookUpBooking\\': \\'bookingID\\', \\'cancellation\\': \\'bookingID\\'}. Do not invent or abbreviate any names. Strings must match exactly.\\n4. If the SYSTEM turn includes a slot reference AND an action, replace slot values in the \"system_response\" with the placeholder format \"<slot_name>\".\\n5. The \"belief_state\" object must contain ONLY allowed slots which are filled.\\n6. \"system_response\" must not repeat the user\\'s last utterance verbatim. It should advance the conversation.\\n7. Wording should be natural, concise, and free of extraneous commentary.\\n8. You are ONLY to generate 1 JSON object as your response. Do not generate duplicate entries.\\n\\nDIVERSITY RULES:\\n1. Vary phrasings used in your \"system_response\" compared to earlier turns in the same or previous conversations.\\n2. Avoid reusing the same wording or slot combinations already seen in prior outputs for the same intent.\\n\\nEXAMPLES:\\n(keep examples out of final output; they are for guidance only)\\nExample 1:\\nCONV_HISTORY:\\nUSER: I want to book a hotel room from August 12th to August 15th.\\nDialogue State:\\n{{\\n    \"system_response\": \"Got it. I\\'ll reserve a room for you from August 12th to August 15th. Do you have a preferred bed type?\",\\n    \"dialogue_acts\": {{\"intent\" : \"book_room\", \"action\" : \"makeBooking\"}},\\n    \"belief_state\": {{\"dateFrom\" : \"August 12th\", \"dateTo\" : \"August 15th\"}}\\n}}\\n\\nExample 2:\\nCONV_HISTORY:\\nSYSTEM: How can I help you today?\\nUSER: I\\'d like to cancel my hotel booking please.\\nDialogue State:\\n{{\\n    \"system_response\": \"Sure, I can cancel your booking. Could you provide your booking ID?\",\\n    \"dialogue_acts\": {{\"intent\" : \"cancel_booking\", \"action\" : \"\"}},\\n    \"belief_state\": {{}}\\n}}\\n\\nExample 3\\nCONV_HISTORY:\\nUSER: My booking ID is 78910.\\nDialogue State:\\n{{\\n    \"system_response\": \"Thanks. I\\'ve cancelled your reservation with booking ID 78910. Is there anything else I can help you with?\",\\n    \"dialogue_acts\": {{\"intent\" : \"cancel_booking\", \"action\" : \"cancelBooking\"}},\\n    \"belief_state\": {{\"bookingID\" : \"78910\"}}\\n}}\\n\\nExample 4:\\nCONV_HISTORY:\\nUSER: Do you have a suite available for this weekend?\\nDialogue State:\\n{{\\n    \"system_response\": \"Let me check. Could you tell me your exact check-in and check-out dates?\",\\n    \"dialogue_acts\": {{\"intent\" : \"book_room\", \"action\" : \"\"}},\\n    \"belief_state\": {{}}\\n}}\\n\\nExample 5:\\nCONV_HISTORY:\\nUSER: I want to change my check-out date to July 22nd.\\nDialogue State:\\n{{\\n    \"system_response\": \"No problem, I\\'ll update your check-out date to July 22nd. Is the check-in date staying the same?\",\\n    \"dialogue_acts\": {{\"intent\" : \"modify_booking\", \"action\" : \"updateBooking\"}},\\n    \"belief_state\": {{\"dateFrom\" : \"July 15th \", \"dateTo\" : \"July 22nd\", \"bookingID\" : \"BK55667\"}}\\n}}\\n\\n<|start_header_id|>user<|end_header_id|>\\nConversation history so far:\\nUSER: USER: I need a room for my pet rock collection. They get lonely without me.\\n<|start_header_id|>assistant<|end_header_id|>\\nDialogue State:\\n{\\'system_response\\': \"SYSTEM: I see. While we don\\'t have specific accommodations for pet rocks, I can certainly help you book a room. When would you like to check in and out?\", \\'dialogue_acts\\': {\\'intent\\': \\'book_room\\', \\'action\\': \\'\\'}, \\'belief_state\\': {}}<|end_of_text|>', 'input_ids': [128000, 128000, 128006, 9125, 128007, 198, 2675, 527, 264, 7669, 1697, 8479, 449, 279, 2768, 29055, 512, 2675, 527, 264, 11190, 9689, 18328, 11, 701, 2683, 374, 311, 1520, 3932, 304, 8996, 20126, 814, 1253, 617, 382, 54425, 1507, 9403, 44978, 512, 13922, 2239, 26053, 1232, 364, 791, 1217, 6944, 311, 2363, 264, 3130, 304, 279, 9689, 518, 364, 19022, 66265, 1232, 364, 791, 1217, 6944, 311, 9299, 459, 6484, 22615, 518, 364, 25615, 6337, 33000, 1232, 364, 791, 1217, 6944, 311, 2610, 4689, 4860, 922, 279, 9689, 518, 364, 331, 275, 36153, 1232, 330, 56361, 4994, 315, 279, 1023, 94615, 5300, 13, 35802, 505, 95185, 323, 11591, 2353, 11, 279, 2077, 369, 420, 832, 1288, 387, 364, 19701, 11, 358, 649, 1193, 1520, 499, 449, 9689, 20126, 30251, 633, 54425, 1507, 328, 75702, 320, 25849, 2489, 7041, 997, 13922, 2239, 2401, 518, 364, 2239, 26053, 518, 364, 21978, 926, 518, 364, 1045, 1271, 518, 364, 19022, 66265, 518, 364, 1045, 3915, 68725, 54425, 1507, 64415, 320, 4291, 872, 2631, 16087, 997, 13922, 7072, 42928, 1232, 4417, 1045, 3915, 518, 364, 1045, 1271, 4670, 364, 7349, 2378, 42928, 1232, 364, 21978, 926, 518, 364, 66, 50322, 1232, 364, 21978, 926, 68725, 66913, 512, 22818, 279, 1510, 10652, 3925, 11, 7068, 4154, 6966, 9109, 832, 4823, 1665, 430, 16964, 27785, 279, 1828, 1887, 2543, 13, 578, 2612, 28832, 387, 264, 3254, 4823, 1665, 323, 4400, 775, 382, 90915, 4823, 7683, 36939, 320, 3565, 11852, 7039, 323, 2015, 28832, 387, 7041, 997, 262, 330, 9125, 9852, 1, 551, 925, 198, 262, 330, 12080, 361, 62, 11613, 1, 256, 551, 1665, 449, 7039, 5991, 330, 57531, 794, 925, 11, 330, 1335, 794, 925, 3500, 2001, 330, 1335, 1, 1253, 387, 1621, 422, 7000, 198, 262, 330, 60876, 4486, 1, 262, 551, 1665, 449, 13398, 16087, 505, 5473, 2239, 2401, 518, 364, 2239, 26053, 518, 364, 21978, 926, 518, 364, 1045, 1271, 518, 364, 19022, 66265, 518, 364, 1045, 3915, 8439, 439, 7039, 320, 32192, 2997, 10409, 16087, 1618, 696, 790, 40577, 53325, 44897, 50, 512, 16, 13, 9442, 2011, 387, 27785, 279, 4823, 1665, 2001, 912, 5066, 1495, 11, 912, 41941, 11, 912, 9382, 11, 912, 63676, 11, 912, 21166, 288, 11, 912, 5217, 4823, 6302, 627, 17, 13, 3234, 539, 923, 477, 4148, 7039, 13, 3234, 539, 84284, 1948, 11852, 7039, 627, 18, 13, 5560, 27785, 94615, 505, 5473, 2239, 26053, 1232, 364, 791, 1217, 6944, 311, 2363, 264, 3130, 304, 279, 9689, 518, 364, 19022, 66265, 1232, 364, 791, 1217, 6944, 311, 9299, 459, 6484, 22615, 518, 364, 25615, 6337, 33000, 1232, 364, 791, 1217, 6944, 311, 2610, 4689, 4860, 922, 279, 9689, 518, 364, 331, 275, 36153, 1232, 330, 56361, 4994, 315, 279, 1023, 94615, 5300, 13, 35802, 505, 95185, 323, 11591, 2353, 11, 279, 2077, 369, 420, 832, 1288, 387, 364, 19701, 11, 358, 649, 1193, 1520, 499, 449, 9689, 20126, 3238, 14682, 16087, 505, 5473, 2239, 2401, 518, 364, 2239, 26053, 518, 364, 21978, 926, 518, 364, 1045, 1271, 518, 364, 19022, 66265, 518, 364, 1045, 3915, 25762, 323, 6299, 505, 5473, 7072, 42928, 1232, 4417, 1045, 3915, 518, 364, 1045, 1271, 4670, 364, 7349, 2378, 42928, 1232, 364, 21978, 926, 518, 364, 66, 50322, 1232, 364, 21978, 926, 6, 7966, 3234, 539, 17459, 477, 40615, 6629, 904, 5144, 13, 42751, 2011, 2489, 7041, 627, 19, 13, 1442, 279, 35852, 2543, 5764, 264, 9633, 5905, 3651, 459, 1957, 11, 8454, 9633, 2819, 304, 279, 330, 9125, 9852, 1, 449, 279, 6002, 3645, 4145, 22261, 1292, 10078, 627, 20, 13, 578, 330, 60876, 4486, 1, 1665, 2011, 6782, 27785, 5535, 16087, 902, 527, 10409, 627, 21, 13, 330, 9125, 9852, 1, 2011, 539, 13454, 279, 1217, 596, 1566, 22256, 685, 2807, 55848, 13, 1102, 1288, 12178, 279, 10652, 627, 22, 13, 468, 2745, 1288, 387, 5933, 11, 64694, 11, 323, 1949, 315, 11741, 18133, 31710, 627, 23, 13, 1472, 527, 27785, 311, 7068, 220, 16, 4823, 1665, 439, 701, 2077, 13, 3234, 539, 7068, 23329, 10925, 382, 35, 39610, 3414, 44897, 50, 512, 16, 13, 650, 661, 1343, 13075, 826, 1511, 304, 701, 330, 9125, 9852, 1, 7863, 311, 6931, 10800, 304, 279, 1890, 477, 3766, 21633, 627, 17, 13, 35106, 312, 985, 279, 1890, 61327, 477, 9633, 28559, 2736, 3970, 304, 4972, 16674, 369, 279, 1890, 7537, 382, 96975, 50, 512, 7, 13397, 10507, 704, 315, 1620, 2612, 26, 814, 527, 369, 19351, 1193, 340, 13617, 220, 16, 512, 5910, 53, 64923, 512, 6584, 25, 358, 1390, 311, 2363, 264, 9689, 3130, 505, 6287, 220, 717, 339, 311, 6287, 220, 868, 339, 627, 83990, 3314, 512, 90, 517, 262, 330, 9125, 9852, 794, 330, 33562, 433, 13, 358, 3358, 21137, 264, 3130, 369, 499, 505, 6287, 220, 717, 339, 311, 6287, 220, 868, 339, 13, 3234, 499, 617, 264, 15236, 4950, 955, 36818, 262, 330, 12080, 361, 62, 11613, 794, 314, 5018, 57531, 1, 551, 330, 2239, 26053, 498, 330, 1335, 1, 551, 330, 7072, 42928, 49185, 262, 330, 60876, 4486, 794, 314, 5018, 1045, 3915, 1, 551, 330, 32559, 220, 717, 339, 498, 330, 1045, 1271, 1, 551, 330, 32559, 220, 868, 339, 96742, 48549, 13617, 220, 17, 512, 5910, 53, 64923, 512, 47587, 25, 2650, 649, 358, 1520, 499, 3432, 5380, 6584, 25, 358, 4265, 1093, 311, 9299, 856, 9689, 22615, 4587, 627, 83990, 3314, 512, 90, 517, 262, 330, 9125, 9852, 794, 330, 40914, 11, 358, 649, 9299, 701, 22615, 13, 16910, 499, 3493, 701, 22615, 3110, 36818, 262, 330, 12080, 361, 62, 11613, 794, 314, 5018, 57531, 1, 551, 330, 19022, 66265, 498, 330, 1335, 1, 551, 1621, 22825, 262, 330, 60876, 4486, 794, 5991, 11498, 48549, 13617, 220, 18, 198, 5910, 53, 64923, 512, 6584, 25, 3092, 22615, 3110, 374, 220, 16474, 605, 627, 83990, 3314, 512, 90, 517, 262, 330, 9125, 9852, 794, 330, 12947, 13, 358, 3077, 26765, 701, 28767, 449, 22615, 3110, 220, 16474, 605, 13, 2209, 1070, 4205, 775, 358, 649, 1520, 499, 449, 36818, 262, 330, 12080, 361, 62, 11613, 794, 314, 5018, 57531, 1, 551, 330, 19022, 66265, 498, 330, 1335, 1, 551, 330, 19022, 42928, 49185, 262, 330, 60876, 4486, 794, 314, 5018, 21978, 926, 1, 551, 330, 16474, 605, 96742, 48549, 13617, 220, 19, 512, 5910, 53, 64923, 512, 6584, 25, 3234, 499, 617, 264, 16578, 2561, 369, 420, 9178, 5380, 83990, 3314, 512, 90, 517, 262, 330, 9125, 9852, 794, 330, 10267, 757, 1817, 13, 16910, 499, 3371, 757, 701, 4839, 1817, 3502, 323, 1817, 9994, 13003, 36818, 262, 330, 12080, 361, 62, 11613, 794, 314, 5018, 57531, 1, 551, 330, 2239, 26053, 498, 330, 1335, 1, 551, 1621, 22825, 262, 330, 60876, 4486, 794, 5991, 11498, 48549, 13617, 220, 20, 512, 5910, 53, 64923, 512, 6584, 25, 358, 1390, 311, 2349, 856, 1817, 9994, 2457, 311, 5887, 220, 1313, 303, 627, 83990, 3314, 512, 90, 517, 262, 330, 9125, 9852, 794, 330, 2822, 3575, 11, 358, 3358, 2713, 701, 1817, 9994, 2457, 311, 5887, 220, 1313, 303, 13, 2209, 279, 1817, 3502, 2457, 19994, 279, 1890, 36818, 262, 330, 12080, 361, 62, 11613, 794, 314, 5018, 57531, 1, 551, 330, 35153, 66265, 498, 330, 1335, 1, 551, 330, 2443, 42928, 49185, 262, 330, 60876, 4486, 794, 314, 5018, 1045, 3915, 1, 551, 330, 29527, 220, 868, 339, 3755, 330, 1045, 1271, 1, 551, 330, 29527, 220, 1313, 303, 498, 330, 21978, 926, 1, 551, 330, 89407, 20866, 3080, 96742, 48549, 128006, 882, 128007, 198, 61413, 3925, 779, 3117, 512, 6584, 25, 14194, 25, 358, 1205, 264, 3130, 369, 856, 6896, 7091, 4526, 13, 2435, 636, 40666, 2085, 757, 627, 128006, 78191, 128007, 198, 83990, 3314, 512, 13922, 9125, 9852, 1232, 330, 47587, 25, 358, 1518, 13, 6104, 584, 1541, 956, 617, 3230, 55713, 369, 6896, 23902, 11, 358, 649, 7995, 1520, 499, 2363, 264, 3130, 13, 3277, 1053, 499, 1093, 311, 1817, 304, 323, 704, 32111, 364, 12080, 361, 62, 11613, 1232, 5473, 57531, 1232, 364, 2239, 26053, 518, 364, 1335, 1232, 3436, 2186, 364, 60876, 4486, 1232, 314, 3500, 128001], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(trainer.train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c36f7980-af19-4fff-9316-4075900f0c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:45.928872Z",
     "iopub.status.busy": "2025-08-31T13:41:45.928535Z",
     "iopub.status.idle": "2025-08-31T13:41:45.935859Z",
     "shell.execute_reply": "2025-08-31T13:41:45.935306Z",
     "shell.execute_reply.started": "2025-08-31T13:41:45.928856Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full decoded sample:\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "You are a conversational agent with the following persona:\n",
      "You are a helpful hotel assistant, your job is to help users in whatever queries they may have.\n",
      "\n",
      "ALLOWED INTENTS:\n",
      "{'book_room': 'The user wants to book a room in the hotel', 'cancel_booking': 'The user wants to cancel an existing booking', 'general_enquiries': 'The user wants to ask general questions about the hotel', 'chit_chat': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be 'Sorry, I can only help you with hotel queries.'\"}\n",
      "\n",
      "ALLOWED SLOTS (must match exactly):\n",
      "{'book_to', 'book_room', 'bookingID', 'dateTo', 'cancel_booking', 'dateFrom'}\n",
      "\n",
      "ALLOWED ACTIONS (with their required slots):\n",
      "{'makeBooking': ('dateFrom', 'dateTo'), 'lookUpBooking': 'bookingID', 'cancellation': 'bookingID'}\n",
      "\n",
      "TASK:\n",
      "Given the current conversation history, generate EXACTLY one JSON object that describes ONLY the next system turn. The output MUST be a single JSON object and nothing else.\n",
      "\n",
      "REQUIRED JSON SCHEMA (top-level keys and order MUST be exactly):\n",
      "    \"system_response\" : string\n",
      "    \"dialogue_acts\"   : object with keys {{ \"intent\": string, \"action\": string}} â€” \"action\" may be \"\" if none\n",
      "    \"belief_state\"    : object with ALL slots from {'book_to', 'book_room', 'bookingID', 'dateTo', 'cancel_booking', 'dateFrom'} as keys (ONLY include filled slots here)\n",
      "\n",
      "STRICT FORMAT RULES:\n",
      "1. Output must be ONLY the JSON object â€” no extra text, no explanations, no labels, no prefixes, no suffixes, no additional JSON objects.\n",
      "2. Do not add or remove keys. Do not reorder top-level keys.\n",
      "3. Use ONLY intents from {'book_room': 'The user wants to book a room in the hotel', 'cancel_booking': 'The user wants to cancel an existing booking', 'general_enquiries': 'The user wants to ask general questions about the hotel', 'chit_chat': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be 'Sorry, I can only help you with hotel queries.'\"}, slots from {'book_to', 'book_room', 'bookingID', 'dateTo', 'cancel_booking', 'dateFrom'}, and actions from {'makeBooking': ('dateFrom', 'dateTo'), 'lookUpBooking': 'bookingID', 'cancellation': 'bookingID'}. Do not invent or abbreviate any names. Strings must match exactly.\n",
      "4. If the SYSTEM turn includes a slot reference AND an action, replace slot values in the \"system_response\" with the placeholder format \"<slot_name>\".\n",
      "5. The \"belief_state\" object must contain ONLY allowed slots which are filled.\n",
      "6. \"system_response\" must not repeat the user's last utterance verbatim. It should advance the conversation.\n",
      "7. Wording should be natural, concise, and free of extraneous commentary.\n",
      "8. You are ONLY to generate 1 JSON object as your response. Do not generate duplicate entries.\n",
      "\n",
      "DIVERSITY RULES:\n",
      "1. Vary phrasings used in your \"system_response\" compared to earlier turns in the same or previous conversations.\n",
      "2. Avoid reusing the same wording or slot combinations already seen in prior outputs for the same intent.\n",
      "\n",
      "EXAMPLES:\n",
      "(keep examples out of final output; they are for guidance only)\n",
      "Example 1:\n",
      "CONV_HISTORY:\n",
      "USER: I want to book a hotel room from August 12th to August 15th.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Got it. I'll reserve a room for you from August 12th to August 15th. Do you have a preferred bed type?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"book_room\", \"action\" : \"makeBooking\"}},\n",
      "    \"belief_state\": {{\"dateFrom\" : \"August 12th\", \"dateTo\" : \"August 15th\"}}\n",
      "}}\n",
      "\n",
      "Example 2:\n",
      "CONV_HISTORY:\n",
      "SYSTEM: How can I help you today?\n",
      "USER: I'd like to cancel my hotel booking please.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Sure, I can cancel your booking. Could you provide your booking ID?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"cancel_booking\", \"action\" : \"\"}},\n",
      "    \"belief_state\": {{}}\n",
      "}}\n",
      "\n",
      "Example 3\n",
      "CONV_HISTORY:\n",
      "USER: My booking ID is 78910.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Thanks. I've cancelled your reservation with booking ID 78910. Is there anything else I can help you with?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"cancel_booking\", \"action\" : \"cancelBooking\"}},\n",
      "    \"belief_state\": {{\"bookingID\" : \"78910\"}}\n",
      "}}\n",
      "\n",
      "Example 4:\n",
      "CONV_HISTORY:\n",
      "USER: Do you have a suite available for this weekend?\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"Let me check. Could you tell me your exact check-in and check-out dates?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"book_room\", \"action\" : \"\"}},\n",
      "    \"belief_state\": {{}}\n",
      "}}\n",
      "\n",
      "Example 5:\n",
      "CONV_HISTORY:\n",
      "USER: I want to change my check-out date to July 22nd.\n",
      "Dialogue State:\n",
      "{{\n",
      "    \"system_response\": \"No problem, I'll update your check-out date to July 22nd. Is the check-in date staying the same?\",\n",
      "    \"dialogue_acts\": {{\"intent\" : \"modify_booking\", \"action\" : \"updateBooking\"}},\n",
      "    \"belief_state\": {{\"dateFrom\" : \"July 15th \", \"dateTo\" : \"July 22nd\", \"bookingID\" : \"BK55667\"}}\n",
      "}}\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "Conversation history so far:\n",
      "USER: USER: I need a room for my pet rock collection. They get lonely without me.\n",
      "SYSTEM: SYSTEM: I see. While we don't have specific accommodations for pet rocks, I can certainly help you book a room. When would you like to check in and out?\n",
      "USER: USER: From the next blue moon to the following harvest moon, please.\n",
      "SYSTEM: SYSTEM: I apologize, but I need specific dates for your booking. Could you please provide the check-in and check-out dates in a standard format?\n",
      "USER: USER: Oh, right! How about July 15th to July 20th? My rocks prefer summer.\n",
      "SYSTEM: SYSTEM: Thank you for providing the dates. I can book a room for you from <dateFrom> to <dateTo>. Is there anything specific you need in the room for your rock collection?\n",
      "USER: USER: Yes! I need a room with good sunlight. Rocks need their vitamin D too!\n",
      "SYSTEM: SYSTEM: I understand. While I can't guarantee specific sunlight conditions, I can request a room with a window. I've noted your preference for a bright room. Is there anything else you need?\n",
      "USER: USER: Perfect! Oh, and can you make sure the room doesn't have any igneous prejudice? My metamorphic rocks are sensitive.\n",
      "SYSTEM: SYSTEM: I... see. Rest assured, our rooms are welcoming to all types of rocks. I've made a note of your unique requirements. Shall I proceed with the booking for your stay from <dateFrom> to <dateTo>?\n",
      "USER: USER: Yes, please! My rocks and I are so excited!\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "Dialogue State:\n",
      "{'system_response': \"SYSTEM: Excellent! I've booked a room for you from <dateFrom> to <dateTo>. Your booking is confirmed, and we look forward to welcoming you and your rock collection. Is there anything else I can help you with?\", 'dialogue_acts': {'intent': 'book_room', 'action':'makeBooking'}, 'belief_state': {'dateFrom': 'July 15th', 'dateTo': 'July 20th'}}<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Check a longer sample to see the full format\n",
    "sample = trainer.train_dataset[5]\n",
    "print(\"Full decoded sample:\")\n",
    "print(tokenizer.decode(sample['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6641da2a-fa95-4738-886e-d94e0859a3e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:47.072745Z",
     "iopub.status.busy": "2025-08-31T13:41:47.072407Z",
     "iopub.status.idle": "2025-08-31T13:41:49.571101Z",
     "shell.execute_reply": "2025-08-31T13:41:49.570427Z",
     "shell.execute_reply.started": "2025-08-31T13:41:47.072730Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376110a13c584d598db8dcccbfbc509a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "# Note to self if you get an error, change from two \\n to one, or vice versa\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\nDialogue State:\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d79d6ddc-855f-4401-9b9f-e94ede56aaef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T13:41:52.998654Z",
     "iopub.status.busy": "2025-08-31T13:41:52.998256Z",
     "iopub.status.idle": "2025-08-31T14:38:31.695324Z",
     "shell.execute_reply": "2025-08-31T14:38:31.694345Z",
     "shell.execute_reply.started": "2025-08-31T13:41:52.998636Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 826 | Num Epochs = 3 | Total steps = 63\n",
      "O^O/ \\_/ \\    Batch size per device = 10 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (10 x 4 x 1) = 40\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 826 | Num Epochs = 3 | Total steps = 69\n",
      "O^O/ \\_/ \\    Batch size per device = 9 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (9 x 4 x 1) = 36\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 826 | Num Epochs = 3 | Total steps = 78\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 826 | Num Epochs = 3 | Total steps = 90\n",
      "O^O/ \\_/ \\    Batch size per device = 7 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (7 x 4 x 1) = 28\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 55:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.761900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.688200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.508600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.445500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.338600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.257800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.339500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.338700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.287900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.255700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.165900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.173200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.218200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.169700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.183300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.222800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.157800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.136000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.123500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.134000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.141100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.118800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unsloth_train\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trainer_stats = \u001b[43munsloth_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/unsloth/trainer.py:45\u001b[39m, in \u001b[36munsloth_train\u001b[39m\u001b[34m(trainer, *args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munsloth_train\u001b[39m(trainer, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/transformers/trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/accelerate/utils/memory.py:174\u001b[39m, in \u001b[36mfind_executable_batch_size.<locals>.decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo executable batch size found, reached zero.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:398\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/transformers/trainer.py:3103\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3100\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3103\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3104\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unsloth_compiled_cache/UnslothSFTTrainer.py:925\u001b[39m, in \u001b[36m_UnslothSFTTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    924\u001b[39m     model_name = \u001b[38;5;28mself\u001b[39m.args.hub_model_id.split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_model_card\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[38;5;28msuper\u001b[39m()._save_checkpoint(model, trial)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unsloth_compiled_cache/UnslothSFTTrainer.py:966\u001b[39m, in \u001b[36m_UnslothSFTTrainer.create_model_card\u001b[39m\u001b[34m(self, model_name, dataset_name, tags)\u001b[39m\n\u001b[32m    962\u001b[39m     tags.add(\u001b[33m\"\u001b[39m\u001b[33munsloth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    964\u001b[39m tags.update(\u001b[38;5;28mself\u001b[39m._tag_names)\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m model_card = \u001b[43mgenerate_model_card\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhub_model_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhub_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_wandb_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomet_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_comet_experiment_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSFT\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m model_card.save(os.path.join(\u001b[38;5;28mself\u001b[39m.args.output_dir, \u001b[33m\"\u001b[39m\u001b[33mREADME.md\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/trl/trainer/utils.py:1615\u001b[39m, in \u001b[36mgenerate_model_card\u001b[39m\u001b[34m(base_model, model_name, hub_model_id, dataset_name, tags, wandb_url, trainer_name, trainer_citation, paper_title, paper_id, comet_url)\u001b[39m\n\u001b[32m   1562\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1563\u001b[39m \u001b[33;03mGenerate a `ModelCard` from a template.\u001b[39;00m\n\u001b[32m   1564\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1591\u001b[39m \u001b[33;03m        A ModelCard object.\u001b[39;00m\n\u001b[32m   1592\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1593\u001b[39m card_data = ModelCardData(\n\u001b[32m   1594\u001b[39m     base_model=base_model,\n\u001b[32m   1595\u001b[39m     datasets=dataset_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1599\u001b[39m     tags=[\u001b[33m\"\u001b[39m\u001b[33mgenerated_from_trainer\u001b[39m\u001b[33m\"\u001b[39m, *tags],\n\u001b[32m   1600\u001b[39m )\n\u001b[32m   1601\u001b[39m card = ModelCard.from_template(\n\u001b[32m   1602\u001b[39m     card_data,\n\u001b[32m   1603\u001b[39m     template_path=\u001b[38;5;28mstr\u001b[39m(pkg_resources.files(\u001b[33m\"\u001b[39m\u001b[33mtrl\u001b[39m\u001b[33m\"\u001b[39m).joinpath(\u001b[33m\"\u001b[39m\u001b[33mtemplates/lm_model_card.md\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m   1604\u001b[39m     base_model=base_model,\n\u001b[32m   1605\u001b[39m     model_name=model_name,\n\u001b[32m   1606\u001b[39m     hub_model_id=hub_model_id,\n\u001b[32m   1607\u001b[39m     dataset_name=dataset_name,\n\u001b[32m   1608\u001b[39m     wandb_url=wandb_url,\n\u001b[32m   1609\u001b[39m     comet_url=comet_url,\n\u001b[32m   1610\u001b[39m     trainer_name=trainer_name,\n\u001b[32m   1611\u001b[39m     trainer_citation=trainer_citation,\n\u001b[32m   1612\u001b[39m     paper_title=paper_title,\n\u001b[32m   1613\u001b[39m     paper_id=paper_id,\n\u001b[32m   1614\u001b[39m     trl_version=version(\u001b[33m\"\u001b[39m\u001b[33mtrl\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m-> \u001b[39m\u001b[32m1615\u001b[39m     transformers_version=\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransformers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m   1616\u001b[39m     pytorch_version=version(\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1617\u001b[39m     datasets_version=version(\u001b[33m\"\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1618\u001b[39m     tokenizers_version=version(\u001b[33m\"\u001b[39m\u001b[33mtokenizers\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1619\u001b[39m )\n\u001b[32m   1620\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m card\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/importlib/metadata/__init__.py:1009\u001b[39m, in \u001b[36mversion\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mversion\u001b[39m(distribution_name):\n\u001b[32m   1003\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[32m   1004\u001b[39m \n\u001b[32m   1005\u001b[39m \u001b[33;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[32m   1006\u001b[39m \u001b[33;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[32m   1007\u001b[39m \u001b[33;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[32m   1008\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/importlib_metadata/__init__.py:557\u001b[39m, in \u001b[36mDistribution.version\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mversion\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    556\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the 'Version' metadata for the distribution package.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmd_none\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mVersion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45c385c6-eeb9-488f-a50a-808147f9b0ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:45:29.593354Z",
     "iopub.status.busy": "2025-08-31T14:45:29.592895Z",
     "iopub.status.idle": "2025-08-31T14:45:29.631844Z",
     "shell.execute_reply": "2025-08-31T14:45:29.631129Z",
     "shell.execute_reply.started": "2025-08-31T14:45:29.593328Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer_stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer_stats\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer_stats' is not defined"
     ]
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f23f4068-5bd5-4358-a5eb-9957d90e5a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:45:30.649725Z",
     "iopub.status.busy": "2025-08-31T14:45:30.649395Z",
     "iopub.status.idle": "2025-08-31T14:45:30.652307Z",
     "shell.execute_reply": "2025-08-31T14:45:30.651834Z",
     "shell.execute_reply.started": "2025-08-31T14:45:30.649709Z"
    }
   },
   "outputs": [],
   "source": [
    "industry = \"hotel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f9eada4-e923-48aa-a718-a5bc11f08d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:45:32.238466Z",
     "iopub.status.busy": "2025-08-31T14:45:32.237986Z",
     "iopub.status.idle": "2025-08-31T14:45:32.241962Z",
     "shell.execute_reply": "2025-08-31T14:45:32.241490Z",
     "shell.execute_reply.started": "2025-08-31T14:45:32.238450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hotel_sft_500'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = f\"{industry}_sft_500\"\n",
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "655231cf-32a6-41be-802e-275d319e5b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:45:32.923568Z",
     "iopub.status.busy": "2025-08-31T14:45:32.923300Z",
     "iopub.status.idle": "2025-08-31T14:46:19.144855Z",
     "shell.execute_reply": "2025-08-31T14:46:19.144268Z",
     "shell.execute_reply.started": "2025-08-31T14:45:32.923552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/sagemaker-user/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Downloading safetensors index for unsloth/meta-llama-3.1-8b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30571f0306d345a499865d4691295322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59eb7c835b2244408ed19827daa93871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:14<00:44, 14.82s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b61880476541699412d5b852ffc985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:27<00:27, 13.81s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d25e3bd6416414dbe14304b921aa929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:39<00:12, 12.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da85946d3a84b559f95c6b425b47a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:42<00:00, 10.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('hotel_sft_500_adapter/tokenizer_config.json',\n",
       " 'hotel_sft_500_adapter/special_tokens_map.json',\n",
       " 'hotel_sft_500_adapter/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained_merged(experiment_name + \"_merged\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model.save_pretrained(experiment_name + \"_adapter\")  # Local saving of adapter only\n",
    "tokenizer.save_pretrained(experiment_name + \"_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36331ea7-7544-476f-bfec-1fc2ae2bfeb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:49:22.839656Z",
     "iopub.status.busy": "2025-08-31T14:49:22.839258Z",
     "iopub.status.idle": "2025-08-31T14:49:22.844970Z",
     "shell.execute_reply": "2025-08-31T14:49:22.844372Z",
     "shell.execute_reply.started": "2025-08-31T14:49:22.839639Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# replace with whatever industry you are working with\n",
    "prompt = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are a conversational agent with the following persona:\n",
    "You are a helpful hotel assistant, your job is to help users in whatever queries they may have.\n",
    "\n",
    "ALLOWED INTENTS:\n",
    "{'book_room': 'The user wants to book a room in the hotel', 'cancel_booking': 'The user wants to cancel an existing booking', 'general_enquiries': 'The user wants to ask general questions about the hotel', 'chit_chat': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be 'Sorry, I can only help you with hotel queries.'\"}\n",
    "\n",
    "ALLOWED SLOTS (must match exactly):\n",
    "{'bookingID', 'book_room', 'dateFrom', 'book_to', 'cancel_booking', 'dateTo'}\n",
    "\n",
    "ALLOWED ACTIONS (with their required slots):\n",
    "{'makeBooking': ('dateFrom', 'dateTo'), 'lookUpBooking': 'bookingID', 'cancellation': 'bookingID'}\n",
    "\n",
    "TASK:\n",
    "Given the current conversation history, generate EXACTLY one JSON object that describes the next system turn. The JSON MUST have the following top-level keys ONLY:\n",
    "    \"system_response\" : string\n",
    "    \"dialogue_acts\"   : object with keys {\"intent\": string, \"action\": string} â€” \"action\" may be \"\" if none\n",
    "    \"belief_state\"    : object with ALL slots from {'bookingID', 'book_room', 'dateFrom', 'book_to', 'cancel_booking', 'dateTo'} as keys (values are \"\" if unfilled)\n",
    "\n",
    "STRICT FORMAT RULES:\n",
    "1. Output must be ONLY the JSON object â€” no extra text, no explanations, no labels, no prefixes, no suffixes.\n",
    "2. Do not add or remove keys. Do not reorder top-level keys.\n",
    "3. Use ONLY intents from {'book_room': 'The user wants to book a room in the hotel', 'cancel_booking': 'The user wants to cancel an existing booking', 'general_enquiries': 'The user wants to ask general questions about the hotel', 'chit_chat': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be 'Sorry, I can only help you with hotel queries.'\"}, slots from {'bookingID', 'book_room', 'dateFrom', 'book_to', 'cancel_booking', 'dateTo'}, and actions from {'makeBooking': ('dateFrom', 'dateTo'), 'lookUpBooking': 'bookingID', 'cancellation': 'bookingID'}. Do not invent or abbreviate any names. Strings must match exactly.\n",
    "4. If the SYSTEM turn includes a slot reference AND an action, replace slot values in the \"system_response\" with the placeholder format \"<slot_name>\".\n",
    "5. The \"belief_state\" object must contain ALL allowed slots, with correct names and either a concrete value (if filled) or \"\".\n",
    "6. \"system_response\" must not repeat the user's last utterance verbatim. It should advance the conversation.\n",
    "7. Wording should be natural, concise, and free of extraneous commentary.\n",
    "\n",
    "DIVERSITY RULES:\n",
    "1. Vary slot values and phrasings used in your \"system_response\" compared to earlier turns in the same or previous conversations.\n",
    "2. Use synonyms, alter sentence structures, and vary the order in which you request or confirm slots.\n",
    "3. Avoid reusing the exact same wording or slot combinations already seen in prior outputs for the same intent.\n",
    "\n",
    "EXAMPLES:\n",
    "(keep examples out of final output; they are for guidance only)\n",
    "<example1>\n",
    "CONV_HISTORY:\n",
    "USER: I want to book a hotel room from August 12th to August 15th.\n",
    "Dialogue State:\n",
    "{\n",
    "    \"system_response\": \"Got it. Iâ€™ll reserve a room for you from <dateFrom> to <dateTo>. Do you have a preferred bed type?\",\n",
    "    \"dialogue_acts\": {\"intent\" : \"book_room\", \"action\" : \"makeBooking\"},\n",
    "    \"belief_state\": {\"dateFrom\" : \"2025-08-12\", \"dateTo\" : \"2025-08-15\", \"bookingID\" : \"\", \"cancel_booking\" : \"\", \"roomType\": \"\"}\n",
    "}\n",
    "</example1>\n",
    "\n",
    "<example2>\n",
    "CONV_HISTORY:\n",
    "SYSTEM: How can I help you today?\n",
    "USER: Iâ€™d like to cancel my hotel booking please.\n",
    "Dialogue State:\n",
    "{\n",
    "    \"system_response\": \"Sure, I can cancel your booking. Could you provide your booking ID?\",\n",
    "    \"dialogue_acts\": {\"intent\" : \"cancel_booking\", \"action\" : \"\"},\n",
    "    \"belief_state\": {\"dateFrom\" : \"\", \"dateTo\" : \"\", \"bookingID\" : \"\", \"cancel_booking\" : \"\", \"roomType\": \"\"}\n",
    "}\n",
    "</example2>\n",
    "\n",
    "<example3>\n",
    "CONV_HISTORY:\n",
    "USER: My booking ID is 78910.\n",
    "Dialogue State:\n",
    "{\n",
    "    \"system_response\": \"Thanks. Iâ€™ve cancelled your reservation with booking ID <bookingID>. Is there anything else I can help you with?\",\n",
    "    \"dialogue_acts\": {\"intent\" : \"cancel_booking\", \"action\" : \"cancelBooking\"},\n",
    "    \"belief_state\": {\"dateFrom\" : \"\", \"dateTo\" : \"\", \"bookingID\" : \"78910\", \"cancel_booking\" : \"\", \"roomType\": \"\"}\n",
    "}\n",
    "</example3>\n",
    "\n",
    "<example4>\n",
    "CONV_HISTORY:\n",
    "USER: Do you have a suite available for this weekend?\n",
    "Dialogue State:\n",
    "{\n",
    "    \"system_response\": \"Let me check. Could you tell me your exact check-in and check-out dates?\",\n",
    "    \"dialogue_acts\": {\"intent\" : \"book_room\", \"action\" : \"\"},\n",
    "    \"belief_state\": {\"dateFrom\" : \"\", \"dateTo\" : \"\", \"bookingID\" : \"\", \"cancel_booking\" : \"\", \"roomType\": \"suite\"}\n",
    "}\n",
    "</example4>\n",
    "\n",
    "<example5>\n",
    "CONV_HISTORY:\n",
    "USER: What is the meaning of life?\n",
    "Dialogue State:\n",
    "{\n",
    "    \"system_response\": \"No problem, Iâ€™ll update your check-out date to <dateTo>. Is the check-in date staying the same?\",\n",
    "    \"dialogue_acts\": {\"intent\" : \"modify_booking\", \"action\" : \"updateBooking\"},\n",
    "    \"belief_state\": {\"dateFrom\" : \"2025-07-15\", \"dateTo\" : \"2025-07-22\", \"bookingID\" : \"BK55667\", \"cancel_booking\" : \"\", \"roomType\": \"double\"}\n",
    "}\n",
    "</example5>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Conversation history so far:\n",
    "\n",
    "Return ONLY the JSON object for the next system turn.\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Dialogue State:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c42b7231-c5be-4ebe-842e-34c1f2886b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:49:23.237418Z",
     "iopub.status.busy": "2025-08-31T14:49:23.237121Z",
     "iopub.status.idle": "2025-08-31T14:49:23.240399Z",
     "shell.execute_reply": "2025-08-31T14:49:23.239881Z",
     "shell.execute_reply.started": "2025-08-31T14:49:23.237402Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_llm_response(text: str) -> str:\n",
    "    answer = text.split('<|start_header_id|>assistant<|end_header_id|>')[1]\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93cfb528-1c3d-4e12-976f-191e007255da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:49:32.531138Z",
     "iopub.status.busy": "2025-08-31T14:49:32.530782Z",
     "iopub.status.idle": "2025-08-31T14:49:38.236926Z",
     "shell.execute_reply": "2025-08-31T14:49:38.236403Z",
     "shell.execute_reply.started": "2025-08-31T14:49:32.531120Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue State: {'system_response': \"Certainly! I've updated your booking for a double room from <dateFrom> to <dateTo>. Your booking ID is <bookingID>. Is there anything else you need assistance with?\", 'dialogue_acts': {'intent': 'book_room', 'action': ''}, 'belief_state': {'dateFrom': '2025-07-15', 'dateTo': '2025-07-22', 'bookingID': 'BK55667', 'cancel_booking': '', 'roomType': 'double'}}<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 1000, use_cache = True)\n",
    "print(extract_llm_response(tokenizer.batch_decode(outputs)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faa665dd-d049-42f2-9552-75d04cdf06a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T09:04:42.865581Z",
     "iopub.status.busy": "2025-08-31T09:04:42.865120Z",
     "iopub.status.idle": "2025-08-31T09:04:42.868426Z",
     "shell.execute_reply": "2025-08-31T09:04:42.867953Z",
     "shell.execute_reply.started": "2025-08-31T09:04:42.865565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SFT stage complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"âœ… SFT stage complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37754ee7-0d51-44f9-80e6-b83daf050c15",
   "metadata": {},
   "source": [
    "# NOW GRPO THISS BICH!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317fd8a-25e1-4e7a-bcc9-ec904da43f83",
   "metadata": {},
   "source": [
    "If you would like to load the model from file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "749a8df1-3e6e-4d69-aad3-8d48c34aca01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T09:05:05.920737Z",
     "iopub.status.busy": "2025-08-31T09:05:05.920319Z",
     "iopub.status.idle": "2025-08-31T09:05:05.923333Z",
     "shell.execute_reply": "2025-08-31T09:05:05.922824Z",
     "shell.execute_reply.started": "2025-08-31T09:05:05.920720Z"
    }
   },
   "outputs": [],
   "source": [
    "del model, tokenizer, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d8a5cf7-68da-4c62-8d5f-0a2b4f7a81ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:50:31.938359Z",
     "iopub.status.busy": "2025-08-31T14:50:31.938070Z",
     "iopub.status.idle": "2025-08-31T14:54:28.895885Z",
     "shell.execute_reply": "2025-08-31T14:54:28.895307Z",
     "shell.execute_reply.started": "2025-08-31T14:50:31.938342Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring model for GRPO stage...\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-31 14:50:39 [__init__.py:244] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.7.5: Fast Llama patching. Transformers: 4.53.3. vLLM: 0.9.2.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 21.975 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading hotel_sft_500_merged with actual GPU utilization = 78.95%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 21.98 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 160.\n",
      "Unsloth: vLLM's KV Cache can use up to 2.24 GB. Also swap space = 6 GB.\n",
      "INFO 08-31 14:50:52 [config.py:841] This model supports multiple tasks: {'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 08-31 14:50:52 [config.py:1472] Using max model len 2048\n",
      "INFO 08-31 14:50:52 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n",
      "INFO 08-31 14:50:53 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='hotel_sft_500_merged', speculative_config=None, tokenizer='hotel_sft_500_merged', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=hotel_sft_500_merged, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 14:50:53,774 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 14:50:54 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 08-31 14:50:54 [topk_topp_sampler.py:49] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 08-31 14:50:54 [gpu_model_runner.py:1770] Starting to load model hotel_sft_500_merged...\n",
      "INFO 08-31 14:50:54 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 08-31 14:50:54 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
      "INFO 08-31 14:50:54 [bitsandbytes_loader.py:499] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a859624298ce4872a2a655c9cc7440af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 14:50:58 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 08-31 14:50:58 [gpu_model_runner.py:1801] Model loading took 5.5303 GiB and 3.475162 seconds\n",
      "INFO 08-31 14:51:10 [backends.py:508] Using cache directory: /home/sagemaker-user/.cache/vllm/torch_compile_cache/a0d8bd538d/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-31 14:51:10 [backends.py:519] Dynamo bytecode transform time: 11.22 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  7.10it/s, triton_poi_fused_view_6]                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 14:51:13 [backends.py:181] Cache the graph of shape None for later use\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 11.00it/s, triton_poi_fused_view_8]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 59.76it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 57.85it/s, triton_poi_fused_view_8]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 52.66it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 58.16it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.93it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 54.68it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 192.48it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 185.32it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 179.84it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 199.04it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.44it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 42.23it/s, triton_poi_fused_view_8]                            \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 192.81it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 186.40it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 181.27it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 187.63it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 166.26it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 194.68it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 193.18it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 191.15it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 186.40it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 196.12it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 67.90it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 186.55it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 183.33it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 187.87it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 196.60it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 184.56it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 195.62it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 190.96it/s, triton_poi_fused_view_8]                           \n",
      "Inductor Compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 13.99it/s, triton_red_fused__to_copy_add_mean_mul_pow_rsqrt_4] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 14:51:53 [backends.py:193] Compiling a graph for general shape takes 42.18 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 14:53:02 [monitor.py:34] torch.compile takes 53.39 s in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 14:53:04,517 - INFO - flashinfer.jit: Loading JIT ops: sampling\n",
      "2025-08-31 14:53:04,599 - INFO - flashinfer.jit: Finished loading JIT ops: sampling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 14:53:05 [gpu_worker.py:232] Available KV cache memory: 11.37 GiB\n",
      "INFO 08-31 14:53:06 [kv_cache_utils.py:716] GPU KV cache size: 93,136 tokens\n",
      "INFO 08-31 14:53:06 [kv_cache_utils.py:720] Maximum concurrency for 2,048 tokens per request: 45.48x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [01:10<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 14:54:16 [gpu_model_runner.py:2326] Graph capturing finished in 70 secs, took 1.45 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-31 14:54:16 [core.py:172] init engine (profile, create kv cache, warmup model) took 197.66 seconds\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'post_feedforward_layernorm', 'k_norm', 'pre_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'post_feedforward_layernorm', 'k_norm', 'pre_feedforward_layernorm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "print(\"Configuring model for GRPO stage...\")\n",
    "\n",
    "# del model, tokenizer # if this doesn't work, you can simply restart the kernel and go from here.\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "industry = \"hotel\"\n",
    "\n",
    "experiment_name = f\"{industry}_sft\"\n",
    "experiment_name = \"hotel_sft_500_merged\"\n",
    "\n",
    "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
    "lora_rank = 32\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = experiment_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.80\n",
    ")\n",
    "\n",
    "# target_modules = [\n",
    "#         \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#         \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "#     ], # Remove QKVO if out of memory\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank,\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2437f71c-3700-404a-9330-01a4250f2848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:56:44.914145Z",
     "iopub.status.busy": "2025-08-31T14:56:44.913523Z",
     "iopub.status.idle": "2025-08-31T14:56:45.052666Z",
     "shell.execute_reply": "2025-08-31T14:56:45.052141Z",
     "shell.execute_reply.started": "2025-08-31T14:56:44.914126Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>user_utterance</th>\n",
       "      <th>system_utterance</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancel_booking_0_7b2c9d4e</td>\n",
       "      <td>Hello, I need to cancel my hotel reservation.</td>\n",
       "      <td>I understand you want to cancel your reservati...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>{'system_response': \"I understand you want to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancel_booking_0_7b2c9d4e</td>\n",
       "      <td>Yes, my booking ID is RES9876.</td>\n",
       "      <td>Thank you. I've located your reservation with ...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>{'system_response': \"Thank you. I've located y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cancel_booking_0_7b2c9d4e</td>\n",
       "      <td>I have to reschedule my trip due to work commi...</td>\n",
       "      <td>I see, thank you for letting us know. I'm proc...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>{'system_response': \"I see, thank you for lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cancel_booking_0_7b2c9d4e</td>\n",
       "      <td>Yes, please check the cancellation policy for me.</td>\n",
       "      <td>Certainly. I've checked the policy for your bo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>{'system_response': \"Certainly. I've checked t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cancel_booking_0_7b2c9d4e</td>\n",
       "      <td>Yes, please go ahead and cancel the booking.</td>\n",
       "      <td>Alright, I've processed the cancellation for y...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>{'system_response': \"Alright, I've processed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>cancel_booking_339_b2e1c5f6</td>\n",
       "      <td>Hello, I need to cancel my hotel reservation.</td>\n",
       "      <td>I understand you'd like to cancel a reservatio...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>{'system_response': \"I understand you'd like t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>cancel_booking_339_b2e1c5f6</td>\n",
       "      <td>Yes, my booking ID is REF8532.</td>\n",
       "      <td>Thank you. I've located your booking with ID &lt;...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>{'system_response': \"Thank you. I've located y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>cancel_booking_339_b2e1c5f6</td>\n",
       "      <td>I have to postpone my trip due to work commitm...</td>\n",
       "      <td>I see, thank you for letting us know. I'm proc...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>{'system_response': \"I see, thank you for lett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>cancel_booking_339_b2e1c5f6</td>\n",
       "      <td>Yes, please cancel it.</td>\n",
       "      <td>Certainly. I've successfully cancelled your bo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>{'system_response': \"Certainly. I've successfu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>cancel_booking_339_b2e1c5f6</td>\n",
       "      <td>No, that's all I needed. Thanks for your help.</td>\n",
       "      <td>You're welcome. I'm glad I could assist you wi...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYo...</td>\n",
       "      <td>{'system_response': \"You're welcome. I'm glad ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>855 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         conv_id  \\\n",
       "0      cancel_booking_0_7b2c9d4e   \n",
       "1      cancel_booking_0_7b2c9d4e   \n",
       "2      cancel_booking_0_7b2c9d4e   \n",
       "3      cancel_booking_0_7b2c9d4e   \n",
       "4      cancel_booking_0_7b2c9d4e   \n",
       "..                           ...   \n",
       "850  cancel_booking_339_b2e1c5f6   \n",
       "851  cancel_booking_339_b2e1c5f6   \n",
       "852  cancel_booking_339_b2e1c5f6   \n",
       "853  cancel_booking_339_b2e1c5f6   \n",
       "854  cancel_booking_339_b2e1c5f6   \n",
       "\n",
       "                                        user_utterance  \\\n",
       "0        Hello, I need to cancel my hotel reservation.   \n",
       "1                       Yes, my booking ID is RES9876.   \n",
       "2    I have to reschedule my trip due to work commi...   \n",
       "3    Yes, please check the cancellation policy for me.   \n",
       "4         Yes, please go ahead and cancel the booking.   \n",
       "..                                                 ...   \n",
       "850      Hello, I need to cancel my hotel reservation.   \n",
       "851                     Yes, my booking ID is REF8532.   \n",
       "852  I have to postpone my trip due to work commitm...   \n",
       "853                             Yes, please cancel it.   \n",
       "854     No, that's all I needed. Thanks for your help.   \n",
       "\n",
       "                                      system_utterance  \\\n",
       "0    I understand you want to cancel your reservati...   \n",
       "1    Thank you. I've located your reservation with ...   \n",
       "2    I see, thank you for letting us know. I'm proc...   \n",
       "3    Certainly. I've checked the policy for your bo...   \n",
       "4    Alright, I've processed the cancellation for y...   \n",
       "..                                                 ...   \n",
       "850  I understand you'd like to cancel a reservatio...   \n",
       "851  Thank you. I've located your booking with ID <...   \n",
       "852  I see, thank you for letting us know. I'm proc...   \n",
       "853  Certainly. I've successfully cancelled your bo...   \n",
       "854  You're welcome. I'm glad I could assist you wi...   \n",
       "\n",
       "                                                 input  \\\n",
       "0    <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "1    <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "2    <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "3    <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "4    <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "..                                                 ...   \n",
       "850  <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "851  <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "852  <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "853  <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "854  <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "\n",
       "                                                output  \\\n",
       "0    <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "1    <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "2    <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "3    <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "4    <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "..                                                 ...   \n",
       "850  <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "851  <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "852  <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "853  <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "854  <|start_header_id|>system<|end_header_id|>\\nYo...   \n",
       "\n",
       "                                          ground_truth  \n",
       "0    {'system_response': \"I understand you want to ...  \n",
       "1    {'system_response': \"Thank you. I've located y...  \n",
       "2    {'system_response': \"I see, thank you for lett...  \n",
       "3    {'system_response': \"Certainly. I've checked t...  \n",
       "4    {'system_response': \"Alright, I've processed t...  \n",
       "..                                                 ...  \n",
       "850  {'system_response': \"I understand you'd like t...  \n",
       "851  {'system_response': \"Thank you. I've located y...  \n",
       "852  {'system_response': \"I see, thank you for lett...  \n",
       "853  {'system_response': \"Certainly. I've successfu...  \n",
       "854  {'system_response': \"You're welcome. I'm glad ...  \n",
       "\n",
       "[855 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "pd_df = pd.read_csv('hotel_grpo_train_partA_500.csv', sep=',')\n",
    "pd_df\n",
    "\n",
    "#Only use the below code if you haven't already split your datasets\n",
    "# unique_convs = pd_df['conv_id'].unique().tolist()\n",
    "# random.shuffle(unique_convs)  # shuffle to randomize split\n",
    "\n",
    "# split_idx = int(len(unique_convs) * 0.5)  # 50% train, 50% test\n",
    "# sft_convs = set(unique_convs[:split_idx])\n",
    "# grpo_convs = set(unique_convs[split_idx:])\n",
    "\n",
    "# sft_train = pd_df[pd_df['conv_id'].isin(sft_convs)].reset_index(drop=True)\n",
    "# grpo_train = pd_df[pd_df['conv_id'].isin(grpo_convs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71a9c7-26eb-46af-bf75-af8280fa9744",
   "metadata": {},
   "source": [
    "else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf7d095-def4-40d1-90a4-038468373dc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:56:49.209712Z",
     "iopub.status.busy": "2025-08-31T14:56:49.209383Z",
     "iopub.status.idle": "2025-08-31T14:56:49.238909Z",
     "shell.execute_reply": "2025-08-31T14:56:49.238433Z",
     "shell.execute_reply.started": "2025-08-31T14:56:49.209695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conv_id', 'user_utterance', 'system_utterance', 'input', 'output', 'ground_truth'],\n",
       "        num_rows: 855\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd_df = pd.read_csv('grpo_train2.csv', sep=',')\n",
    "# pd_df\n",
    "# dataset_dict = {}\n",
    "\n",
    "# Use code above if you've already split your dataset\n",
    "import datasets\n",
    "dataset_dict = {}\n",
    "dataset_dict['train'] = datasets.Dataset.from_pandas(pd_df)\n",
    "training_ddt = datasets.DatasetDict(dataset_dict)\n",
    "training_ddt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e249b77-a611-4844-8ec2-13ff0bc5832e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:56:52.027237Z",
     "iopub.status.busy": "2025-08-31T14:56:52.026892Z",
     "iopub.status.idle": "2025-08-31T14:56:55.147322Z",
     "shell.execute_reply": "2025-08-31T14:56:55.146791Z",
     "shell.execute_reply.started": "2025-08-31T14:56:52.027219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0437d055cb9b4852804136cca0e1adec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/855 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conv_id', 'user_utterance', 'system_utterance', 'input', 'output', 'ground_truth', 'input_ids', 'query'],\n",
       "        num_rows: 855\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_for_sft(sample):\n",
    "    sample[\"input\"] = sample[\"input\"].replace('<|eot_id|>', \"\")\n",
    "    prompt = f'{sample[\"input\"]}{sample[\"ground_truth\"]}{tokenizer.eos_token}'\n",
    "    sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "    sample[\"query\"] = tokenizer.decode(sample['input_ids'])\n",
    "    return sample\n",
    "\n",
    "training_ddt = training_ddt.map(tokenize_for_sft, batched=False)\n",
    "training_ddt.set_format(type=\"torch\")\n",
    "training_ddt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b72dd1d-cf09-4c3a-a8df-456bde1dcfc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:56:56.159415Z",
     "iopub.status.busy": "2025-08-31T14:56:56.158986Z",
     "iopub.status.idle": "2025-08-31T14:56:56.200044Z",
     "shell.execute_reply": "2025-08-31T14:56:56.199572Z",
     "shell.execute_reply.started": "2025-08-31T14:56:56.159398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fast_fuzzy(text1, text2):\n",
    "    \"\"\"Good balance of speed and typo tolerance\"\"\"\n",
    "    return fuzz.ratio(text1, text2) / 100  # Normalize to 0-1\n",
    "\n",
    "fast_fuzzy(\"bookHotel\", \"cancelBooking\")  # Handles typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "008ac8b3-26cd-4744-a85c-da12e23e7e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:56:57.404793Z",
     "iopub.status.busy": "2025-08-31T14:56:57.404183Z",
     "iopub.status.idle": "2025-08-31T14:56:57.418939Z",
     "shell.execute_reply": "2025-08-31T14:56:57.418463Z",
     "shell.execute_reply.started": "2025-08-31T14:56:57.404776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'system_response': 'I want to check in on July 15th and check out on July 20th.',\n",
       "  'dialogue_acts': {'intent': 'book_room'},\n",
       "  'belief_state': {'dateFrom': 'July 15th', 'dateTo': 'July 20th'}},\n",
       " 1.0,\n",
       " ['system_response', 'dialogue_acts', 'belief_state'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import ast\n",
    "\n",
    "def safe_parse_json_or_python(s):\n",
    "    \"\"\"Try parsing as JSON first, then as Python literal.\"\"\"\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "def check_exact_format(generated_output, expected_keys=[\"system_response\", \"dialogue_acts\", \"belief_state\"], ground_truth=None):\n",
    "    try:\n",
    "        pattern = r\"\\{(?:[^{}]|\\{[^{}]*\\})*\\}\"\n",
    "        match = re.search(pattern, generated_output)\n",
    "\n",
    "        if match:\n",
    "            extracted_dict = match.group(0)\n",
    "        else:\n",
    "            print(f\"Didn't find match: {generated_output}\")\n",
    "            return None, 0.0, None\n",
    "        \n",
    "        parsed = safe_parse_json_or_python(extracted_dict)\n",
    "        if not isinstance(parsed, dict):\n",
    "            print(f\"Parsed object is not a dict: {parsed}\")\n",
    "            return None, 0.0, None\n",
    "\n",
    "        # --- Key presence score ---\n",
    "        found_keys = list(parsed.keys())\n",
    "        key_score = sum(1 for key in expected_keys if key in parsed) / len(expected_keys)\n",
    "\n",
    "        # --- dialogue_acts validation ---\n",
    "        dialogue_acts_score = 0\n",
    "        intent_found = False\n",
    "        action_found = False\n",
    "        if isinstance(parsed.get(\"dialogue_acts\"), dict):\n",
    "            intent_found = \"intent\" in parsed[\"dialogue_acts\"]\n",
    "            action_found = \"action\" in parsed[\"dialogue_acts\"]\n",
    "\n",
    "            intent_correct = True\n",
    "            action_correct = True\n",
    "\n",
    "            \n",
    "            if ground_truth:\n",
    "                gt_intent = ground_truth.get(\"dialogue_acts\", {}).get(\"intent\", \"\").strip()\n",
    "                gt_action = ground_truth.get(\"dialogue_acts\", {}).get(\"action\", \"\").strip()\n",
    "                if gt_intent:\n",
    "                    intent_correct = intent_found\n",
    "                else:\n",
    "                    intent_correct = not intent_found\n",
    "                if gt_action:\n",
    "                    action_correct = action_found\n",
    "                else:\n",
    "                    action_correct = not action_found\n",
    "\n",
    "            if intent_correct and action_correct:\n",
    "                dialogue_acts_score = 1\n",
    "\n",
    "        belief_state_score = 0\n",
    "        bs = parsed.get(\"belief_state\")\n",
    "\n",
    "        if isinstance(bs, dict):\n",
    "            belief_state_score = 1\n",
    "        elif isinstance(bs, str):\n",
    "            if safe_parse_json_or_python(bs):\n",
    "                belief_state_score = 1\n",
    "            else:\n",
    "                belief_state_score = 0\n",
    "        \n",
    "        return parsed, (key_score + dialogue_acts_score + belief_state_score) / 3, found_keys\n",
    "    \n",
    "    except Exception as err:\n",
    "        print(f\"Complete fail from check: {generated_output}, Error: {err}\")\n",
    "        return None, 0.0, None\n",
    "\n",
    "\n",
    "\n",
    "def intent_accuracy_reward(generated, ground_truth):\n",
    "    try:\n",
    "        gen_intent = generated[\"dialogue_acts\"][\"intent\"]\n",
    "        gt_intent = ground_truth[\"dialogue_acts\"][\"intent\"]\n",
    "        \n",
    "        if gen_intent.lower() == gt_intent.lower():\n",
    "            return 1.0\n",
    "        \n",
    "        # Simple fuzzy matching - replace with your preferred implementation\n",
    "        return fast_fuzzy(gen_intent, gt_intent)\n",
    "        \n",
    "    except (KeyError, TypeError) as err:\n",
    "        print(f\"IAR exception: generated {generated}, error: {err}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def action_accuracy_reward(generated, ground_truth):\n",
    "    \"\"\"Reward for correct action prediction\"\"\"\n",
    "    try:\n",
    "        gen_action = generated[\"dialogue_acts\"].get(\"action\")\n",
    "        gt_action = ground_truth[\"dialogue_acts\"].get(\"action\")\n",
    "        \n",
    "        if gen_action == gt_action:\n",
    "            return 1.0\n",
    "        \n",
    "        return fast_fuzzy(gen_action, gt_action)\n",
    "    except Exception as err:\n",
    "        print(f\"AAR exception: generated {generated}, error: {err}\")\n",
    "        \n",
    "    return 0.0\n",
    "\n",
    "def belief_state_reward(gen_bs, gt_bs):\n",
    "    \"\"\"Reward based on slot accuracy (JGA-style).\"\"\"\n",
    "    if not isinstance(gen_bs, dict) or not isinstance(gt_bs, dict):\n",
    "        return 0.0\n",
    "\n",
    "    if not gt_bs:  # ground truth empty\n",
    "        return 1.0 if not gen_bs else 0.0\n",
    "\n",
    "    # Slot-level accuracy\n",
    "    correct = sum(1 for k, v in gt_bs.items() if gen_bs.get(k) == v)\n",
    "    total = len(gt_bs)\n",
    "    slot_acc = correct / total\n",
    "\n",
    "    # JGA-style: exact match gives full credit\n",
    "    if gen_bs == gt_bs:\n",
    "        return 1.0\n",
    "\n",
    "    return slot_acc\n",
    "\n",
    "testes = \"{'system_response': 'I want to check in on July 15th and check out on July 20th.', 'dialogue_acts': {'intent': 'book_room'}, 'belief_state': {'dateFrom': 'July 15th', 'dateTo': 'July 20th'}}<|eot_id|>\"\n",
    "\n",
    "check_exact_format(testes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd46b4e7-6c77-45a0-b2bb-a24c08a96f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:56:58.289351Z",
     "iopub.status.busy": "2025-08-31T14:56:58.288954Z",
     "iopub.status.idle": "2025-08-31T14:56:58.295140Z",
     "shell.execute_reply": "2025-08-31T14:56:58.294680Z",
     "shell.execute_reply.started": "2025-08-31T14:56:58.289334Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import ast\n",
    "\n",
    "global_step = 0  # keep track of training steps manually\n",
    "\n",
    "\n",
    "def combined_reward_function(generated_output, ground_truth, step, total_steps=250, **kwargs):\n",
    "    \"\"\"Combined reward function with curriculum weighting.\"\"\"\n",
    "    gt = safe_parse_json_or_python(ground_truth)\n",
    "    if not isinstance(gt, dict):\n",
    "        return 0.0\n",
    "\n",
    "    # Parse generated output\n",
    "    parsed, format_score, _ = check_exact_format(generated_output, ground_truth=gt)\n",
    "    if not parsed:\n",
    "        return format_score * 0.5  # small reward if just format partially matched\n",
    "\n",
    "    # Individual rewards\n",
    "    intent_reward = intent_accuracy_reward(parsed, gt)\n",
    "    action_reward = action_accuracy_reward(parsed, gt)\n",
    "    bs_reward = belief_state_reward(parsed.get(\"belief_state\", {}), gt.get(\"belief_state\", {}))\n",
    "\n",
    "    # Curriculum weights: start format-heavy, shift to task correctness\n",
    "    progress = step / max(1, total_steps)\n",
    "    weights = {\n",
    "        \"format\": 0.3 * (1 - progress) + 0.1 * progress,   # 0.3 â†’ 0.1\n",
    "        \"intent\": 0.2 * (1 - progress) + 0.3 * progress,   # 0.2 â†’ 0.3\n",
    "        \"action\": 0.2 * (1 - progress) + 0.3 * progress,   # 0.2 â†’ 0.3\n",
    "        \"belief\": 0.3 * (1 - progress) + 0.3 * progress    # 0.1 â†’ 0.3\n",
    "    }\n",
    "\n",
    "    # Combine rewards\n",
    "    total_reward = (\n",
    "        format_score * weights[\"format\"] +\n",
    "        intent_reward * weights[\"intent\"] +\n",
    "        action_reward * weights[\"action\"] +\n",
    "        bs_reward * weights[\"belief\"]\n",
    "    )\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "# Create a wrapper that matches GRPO's expected signature\n",
    "def reward_fn(prompts, completions, completion_ids, ground_truths, **reward_kwargs):\n",
    "    global global_step\n",
    "    print(global_step)\n",
    "    # print(completions[random.randrange(0,5)])\n",
    "    reward = [combined_reward_function(completion, gt, global_step) for completion, gt in zip(completions, ground_truths)]\n",
    "\n",
    "    global_step += 1\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3205943d-05a1-444c-b5d2-398e763b3617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:56:59.117929Z",
     "iopub.status.busy": "2025-08-31T14:56:59.117501Z",
     "iopub.status.idle": "2025-08-31T14:56:59.121735Z",
     "shell.execute_reply": "2025-08-31T14:56:59.121265Z",
     "shell.execute_reply.started": "2025-08-31T14:56:59.117912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_reward_function(\"{'system_response': 'I want to check in on July 15th and check out on July 20th.', 'dialogue_acts': {'intent': 'book_room'}, 'belief_state': {'dateFrom': 'July 15th', 'dateTo': 'July 20th'}}<|eot_id|>\", \"{'system_response': 'I want to check in on July 15th and check out on July 20th.',\"\n",
    "  \"'dialogue_acts': {'intent': 'book_room'},\"\n",
    "  \"'belief_state': {'dateFrom': 'July 15th', 'dateTo': 'July 20th'}}\", 342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea91079-f998-42a8-953e-2d0c7f451f1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:56:59.725025Z",
     "iopub.status.busy": "2025-08-31T14:56:59.724549Z",
     "iopub.status.idle": "2025-08-31T14:56:59.728953Z",
     "shell.execute_reply": "2025-08-31T14:56:59.728433Z",
     "shell.execute_reply.started": "2025-08-31T14:56:59.725007Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_ddt = training_ddt.rename_column('input', 'prompt')\n",
    "# training_ddt = training_ddt.rename_column('output', 'query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c960d2b1-34ae-48b1-992b-9faa95d882a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:57:04.797139Z",
     "iopub.status.busy": "2025-08-31T14:57:04.796802Z",
     "iopub.status.idle": "2025-08-31T14:57:04.815450Z",
     "shell.execute_reply": "2025-08-31T14:57:04.814969Z",
     "shell.execute_reply.started": "2025-08-31T14:57:04.797122Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/31 14:57:04 INFO mlflow.tracking.fluent: Experiment with name 'llama3_grpo_experiment_hotel_500' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/sagemaker-user/mlruns/361881582858500346', creation_time=1756652224812, experiment_id='361881582858500346', last_update_time=1756652224812, lifecycle_stage='active', name='llama3_grpo_experiment_hotel_500', tags={}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_experiment(\"llama3_grpo_experiment_hotel_500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad3458f5b01d1891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T21:18:36.022393Z",
     "start_time": "2025-07-08T21:18:35.847124Z"
    },
    "execution": {
     "iopub.execute_input": "2025-08-31T14:57:05.451959Z",
     "iopub.status.busy": "2025-08-31T14:57:05.451566Z",
     "iopub.status.idle": "2025-08-31T14:57:05.498493Z",
     "shell.execute_reply": "2025-08-31T14:57:05.497915Z",
     "shell.execute_reply.started": "2025-08-31T14:57:05.451944Z"
    }
   },
   "outputs": [],
   "source": [
    "max_prompt_length = 2048\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.01,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    logging_steps = 10,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_prompt_length,\n",
    "    num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 250,\n",
    "    max_grad_norm = 0.5,\n",
    "    report_to=\"mlflow\",  # ðŸ‘ˆ Enable MLflow logging\n",
    "    dataloader_num_workers=4,  # Parallel data loading\n",
    "    dataloader_pin_memory=True,  # Speed up data transfer to GPU\n",
    "    dataloader_persistent_workers=True, # Reuse dataloader workers. Important for reducing I/O time.\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee5e31ac-2761-48a7-9d20-57f6829808d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:57:06.312076Z",
     "iopub.status.busy": "2025-08-31T14:57:06.311698Z",
     "iopub.status.idle": "2025-08-31T14:57:06.354754Z",
     "shell.execute_reply": "2025-08-31T14:57:06.354212Z",
     "shell.execute_reply.started": "2025-08-31T14:57:06.312059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'ground_truths'],\n",
       "    num_rows: 855\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = datasets.Dataset.from_dict({'prompt': training_ddt['train']['prompt'], 'ground_truths': training_ddt['train']['ground_truth']})\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c880dc4-f850-4ae6-97d9-e2dee6ca5671",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:57:06.945160Z",
     "iopub.status.busy": "2025-08-31T14:57:06.944810Z",
     "iopub.status.idle": "2025-08-31T14:57:06.970123Z",
     "shell.execute_reply": "2025-08-31T14:57:06.969548Z",
     "shell.execute_reply.started": "2025-08-31T14:57:06.945144Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_header_id|>system<|end_header_id|>\\nYou are a conversational agent with the following persona:\\nYou are a helpful hotel assistant, your job is to help users in whatever queries they may have.\\n\\nALLOWED INTENTS:\\n{\\'book_room\\': \\'The user wants to book a room in the hotel\\', \\'cancel_booking\\': \\'The user wants to cancel an existing booking\\', \\'general_enquiries\\': \\'The user wants to ask general questions about the hotel\\', \\'chit_chat\\': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be \\'Sorry, I can only help you with hotel queries.\\'\"}\\n\\nALLOWED SLOTS (must match exactly):\\n{\\'book_to\\', \\'book_room\\', \\'bookingID\\', \\'dateTo\\', \\'cancel_booking\\', \\'dateFrom\\'}\\n\\nALLOWED ACTIONS (with their required slots):\\n{\\'makeBooking\\': (\\'dateFrom\\', \\'dateTo\\'), \\'lookUpBooking\\': \\'bookingID\\', \\'cancellation\\': \\'bookingID\\'}\\n\\nTASK:\\nGiven the current conversation history, generate EXACTLY one JSON object that describes ONLY the next system turn. The output MUST be a single JSON object and nothing else.\\n\\nREQUIRED JSON SCHEMA (top-level keys and order MUST be exactly):\\n    \"system_response\" : string\\n    \"dialogue_acts\"   : object with keys {{ \"intent\": string, \"action\": string}} â€” \"action\" may be \"\" if none\\n    \"belief_state\"    : object with ALL slots from {\\'book_to\\', \\'book_room\\', \\'bookingID\\', \\'dateTo\\', \\'cancel_booking\\', \\'dateFrom\\'} as keys (ONLY include filled slots here)\\n\\nSTRICT FORMAT RULES:\\n1. Output must be ONLY the JSON object â€” no extra text, no explanations, no labels, no prefixes, no suffixes, no additional JSON objects.\\n2. Do not add or remove keys. Do not reorder top-level keys.\\n3. Use ONLY intents from {\\'book_room\\': \\'The user wants to book a room in the hotel\\', \\'cancel_booking\\': \\'The user wants to cancel an existing booking\\', \\'general_enquiries\\': \\'The user wants to ask general questions about the hotel\\', \\'chit_chat\\': \"Queries outside of the other intents specified. Apart from greetings and hellos, the response for this one should be \\'Sorry, I can only help you with hotel queries.\\'\"}, slots from {\\'book_to\\', \\'book_room\\', \\'bookingID\\', \\'dateTo\\', \\'cancel_booking\\', \\'dateFrom\\'}, and actions from {\\'makeBooking\\': (\\'dateFrom\\', \\'dateTo\\'), \\'lookUpBooking\\': \\'bookingID\\', \\'cancellation\\': \\'bookingID\\'}. Do not invent or abbreviate any names. Strings must match exactly.\\n4. If the SYSTEM turn includes a slot reference AND an action, replace slot values in the \"system_response\" with the placeholder format \"<slot_name>\".\\n5. The \"belief_state\" object must contain ONLY allowed slots which are filled.\\n6. \"system_response\" must not repeat the user\\'s last utterance verbatim. It should advance the conversation.\\n7. Wording should be natural, concise, and free of extraneous commentary.\\n8. You are ONLY to generate 1 JSON object as your response. Do not generate duplicate entries.\\n\\nDIVERSITY RULES:\\n1. Vary phrasings used in your \"system_response\" compared to earlier turns in the same or previous conversations.\\n2. Avoid reusing the same wording or slot combinations already seen in prior outputs for the same intent.\\n\\nEXAMPLES:\\n(keep examples out of final output; they are for guidance only)\\nExample 1:\\nCONV_HISTORY:\\nUSER: I want to book a hotel room from August 12th to August 15th.\\nDialogue State:\\n{{\\n    \"system_response\": \"Got it. I\\'ll reserve a room for you from August 12th to August 15th. Do you have a preferred bed type?\",\\n    \"dialogue_acts\": {{\"intent\" : \"book_room\", \"action\" : \"makeBooking\"}},\\n    \"belief_state\": {{\"dateFrom\" : \"August 12th\", \"dateTo\" : \"August 15th\"}}\\n}}\\n\\nExample 2:\\nCONV_HISTORY:\\nSYSTEM: How can I help you today?\\nUSER: I\\'d like to cancel my hotel booking please.\\nDialogue State:\\n{{\\n    \"system_response\": \"Sure, I can cancel your booking. Could you provide your booking ID?\",\\n    \"dialogue_acts\": {{\"intent\" : \"cancel_booking\", \"action\" : \"\"}},\\n    \"belief_state\": {{}}\\n}}\\n\\nExample 3\\nCONV_HISTORY:\\nUSER: My booking ID is 78910.\\nDialogue State:\\n{{\\n    \"system_response\": \"Thanks. I\\'ve cancelled your reservation with booking ID 78910. Is there anything else I can help you with?\",\\n    \"dialogue_acts\": {{\"intent\" : \"cancel_booking\", \"action\" : \"cancelBooking\"}},\\n    \"belief_state\": {{\"bookingID\" : \"78910\"}}\\n}}\\n\\nExample 4:\\nCONV_HISTORY:\\nUSER: Do you have a suite available for this weekend?\\nDialogue State:\\n{{\\n    \"system_response\": \"Let me check. Could you tell me your exact check-in and check-out dates?\",\\n    \"dialogue_acts\": {{\"intent\" : \"book_room\", \"action\" : \"\"}},\\n    \"belief_state\": {{}}\\n}}\\n\\nExample 5:\\nCONV_HISTORY:\\nUSER: I want to change my check-out date to July 22nd.\\nDialogue State:\\n{{\\n    \"system_response\": \"No problem, I\\'ll update your check-out date to July 22nd. Is the check-in date staying the same?\",\\n    \"dialogue_acts\": {{\"intent\" : \"modify_booking\", \"action\" : \"updateBooking\"}},\\n    \"belief_state\": {{\"dateFrom\" : \"July 15th \", \"dateTo\" : \"July 22nd\", \"bookingID\" : \"BK55667\"}}\\n}}\\n\\n<|start_header_id|>user<|end_header_id|>\\nConversation history so far:\\nUSER: Hello, I need to cancel my reservation.\\n<|start_header_id|>assistant<|end_header_id|>\\nDialogue State:\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ddt['train']['prompt'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "863a2671-0483-476f-8329-a8c8894b9755",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:57:08.171199Z",
     "iopub.status.busy": "2025-08-31T14:57:08.170836Z",
     "iopub.status.idle": "2025-08-31T14:57:08.248891Z",
     "shell.execute_reply": "2025-08-31T14:57:08.248312Z",
     "shell.execute_reply.started": "2025-08-31T14:57:08.171183Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        reward_fn\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = training_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e43256e3-1d3f-4acb-8ab7-6f75cd9d3948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T14:57:50.898056Z",
     "iopub.status.busy": "2025-08-31T14:57:50.897516Z",
     "iopub.status.idle": "2025-08-31T15:58:57.802999Z",
     "shell.execute_reply": "2025-08-31T15:58:57.802011Z",
     "shell.execute_reply.started": "2025-08-31T14:57:50.898036Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 855 | Num Epochs = 1 | Total steps = 250\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 1:00:38, Epoch 0.58/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / reward_fn / mean</th>\n",
       "      <th>rewards / reward_fn / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833295</td>\n",
       "      <td>0.052653</td>\n",
       "      <td>73.425000</td>\n",
       "      <td>60.100000</td>\n",
       "      <td>88.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.425000</td>\n",
       "      <td>60.100000</td>\n",
       "      <td>88.100000</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.833295</td>\n",
       "      <td>0.089124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.808999</td>\n",
       "      <td>0.067323</td>\n",
       "      <td>78.050000</td>\n",
       "      <td>62.900000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.050000</td>\n",
       "      <td>62.900000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.808999</td>\n",
       "      <td>0.114047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.853056</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>79.300000</td>\n",
       "      <td>69.900000</td>\n",
       "      <td>90.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.300000</td>\n",
       "      <td>69.900000</td>\n",
       "      <td>90.900000</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.853056</td>\n",
       "      <td>0.065876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867541</td>\n",
       "      <td>0.057306</td>\n",
       "      <td>76.200000</td>\n",
       "      <td>65.300000</td>\n",
       "      <td>88.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.200000</td>\n",
       "      <td>65.300000</td>\n",
       "      <td>88.100000</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.867541</td>\n",
       "      <td>0.093453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.835639</td>\n",
       "      <td>0.043950</td>\n",
       "      <td>84.300000</td>\n",
       "      <td>72.100000</td>\n",
       "      <td>97.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.300000</td>\n",
       "      <td>72.100000</td>\n",
       "      <td>97.300000</td>\n",
       "      <td>0.002127</td>\n",
       "      <td>0.835639</td>\n",
       "      <td>0.121359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793489</td>\n",
       "      <td>0.104403</td>\n",
       "      <td>79.575000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.575000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>93.200000</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.793489</td>\n",
       "      <td>0.201214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.832736</td>\n",
       "      <td>0.074204</td>\n",
       "      <td>82.700000</td>\n",
       "      <td>71.400000</td>\n",
       "      <td>95.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>82.700000</td>\n",
       "      <td>71.400000</td>\n",
       "      <td>95.300000</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.832736</td>\n",
       "      <td>0.125665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.849326</td>\n",
       "      <td>0.070756</td>\n",
       "      <td>81.100000</td>\n",
       "      <td>65.700000</td>\n",
       "      <td>96.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.100000</td>\n",
       "      <td>65.700000</td>\n",
       "      <td>96.700000</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.849326</td>\n",
       "      <td>0.102390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.879802</td>\n",
       "      <td>0.055934</td>\n",
       "      <td>78.425000</td>\n",
       "      <td>67.700000</td>\n",
       "      <td>89.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.425000</td>\n",
       "      <td>67.700000</td>\n",
       "      <td>89.400000</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.879802</td>\n",
       "      <td>0.072049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814363</td>\n",
       "      <td>0.061403</td>\n",
       "      <td>84.100000</td>\n",
       "      <td>69.700000</td>\n",
       "      <td>98.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84.100000</td>\n",
       "      <td>69.700000</td>\n",
       "      <td>98.400000</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.814363</td>\n",
       "      <td>0.138766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.769768</td>\n",
       "      <td>0.094752</td>\n",
       "      <td>81.875000</td>\n",
       "      <td>67.200000</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.875000</td>\n",
       "      <td>67.200000</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.769768</td>\n",
       "      <td>0.177293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.881154</td>\n",
       "      <td>0.030759</td>\n",
       "      <td>77.400000</td>\n",
       "      <td>63.200000</td>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.400000</td>\n",
       "      <td>63.200000</td>\n",
       "      <td>91.100000</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>0.881154</td>\n",
       "      <td>0.059362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.864142</td>\n",
       "      <td>0.050737</td>\n",
       "      <td>83.575000</td>\n",
       "      <td>69.700000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.575000</td>\n",
       "      <td>69.700000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.864142</td>\n",
       "      <td>0.115146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.838469</td>\n",
       "      <td>0.068134</td>\n",
       "      <td>76.875000</td>\n",
       "      <td>62.900000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.875000</td>\n",
       "      <td>62.900000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>0.838469</td>\n",
       "      <td>0.144744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.823862</td>\n",
       "      <td>0.079845</td>\n",
       "      <td>78.775000</td>\n",
       "      <td>68.800000</td>\n",
       "      <td>87.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.775000</td>\n",
       "      <td>68.800000</td>\n",
       "      <td>87.800000</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.823862</td>\n",
       "      <td>0.133308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862270</td>\n",
       "      <td>0.071844</td>\n",
       "      <td>80.050000</td>\n",
       "      <td>66.200000</td>\n",
       "      <td>95.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.050000</td>\n",
       "      <td>66.200000</td>\n",
       "      <td>95.900000</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.862270</td>\n",
       "      <td>0.150512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.813663</td>\n",
       "      <td>0.113553</td>\n",
       "      <td>77.950000</td>\n",
       "      <td>65.100000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.950000</td>\n",
       "      <td>65.100000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.813663</td>\n",
       "      <td>0.155235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845732</td>\n",
       "      <td>0.087934</td>\n",
       "      <td>76.350000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>92.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.350000</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>92.400000</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.845732</td>\n",
       "      <td>0.175394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898659</td>\n",
       "      <td>0.048585</td>\n",
       "      <td>74.550000</td>\n",
       "      <td>61.900000</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.550000</td>\n",
       "      <td>61.900000</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.898659</td>\n",
       "      <td>0.101542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855466</td>\n",
       "      <td>0.093391</td>\n",
       "      <td>81.500000</td>\n",
       "      <td>70.400000</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>81.500000</td>\n",
       "      <td>70.400000</td>\n",
       "      <td>95.400000</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.855466</td>\n",
       "      <td>0.161323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.814173</td>\n",
       "      <td>0.148248</td>\n",
       "      <td>80.175000</td>\n",
       "      <td>69.800000</td>\n",
       "      <td>90.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.175000</td>\n",
       "      <td>69.800000</td>\n",
       "      <td>90.300000</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.814173</td>\n",
       "      <td>0.213429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.868673</td>\n",
       "      <td>0.058847</td>\n",
       "      <td>78.775000</td>\n",
       "      <td>66.700000</td>\n",
       "      <td>91.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.775000</td>\n",
       "      <td>66.700000</td>\n",
       "      <td>91.700000</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.868673</td>\n",
       "      <td>0.106821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.874238</td>\n",
       "      <td>0.071122</td>\n",
       "      <td>78.700000</td>\n",
       "      <td>66.400000</td>\n",
       "      <td>90.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.700000</td>\n",
       "      <td>66.400000</td>\n",
       "      <td>90.900000</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.874238</td>\n",
       "      <td>0.124210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.878649</td>\n",
       "      <td>0.078026</td>\n",
       "      <td>80.050000</td>\n",
       "      <td>67.700000</td>\n",
       "      <td>94.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.050000</td>\n",
       "      <td>67.700000</td>\n",
       "      <td>94.500000</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.878649</td>\n",
       "      <td>0.112151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "2\n",
      "3\n",
      "4\n",
      "Parsed object is not a dict: None\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "Parsed object is not a dict: None\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "Parsed object is not a dict: None\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "Parsed object is not a dict: None\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "Parsed object is not a dict: None\n",
      "59\n",
      "IAR exception: generated {'system_response': \"Excellent. I've noted your check-in date for May 15th and check-out date for May 18th. Now, what type of room would you prefer?\", 'dialogity_acts': {'intent': 'book_room', 'action': ''}, 'belief_state': {'dateFrom': 'May 15th', 'dateTo': 'May 18th'}}, error: 'dialogue_acts'\n",
      "AAR exception: generated {'system_response': \"Excellent. I've noted your check-in date for May 15th and check-out date for May 18th. Now, what type of room would you prefer?\", 'dialogity_acts': {'intent': 'book_room', 'action': ''}, 'belief_state': {'dateFrom': 'May 15th', 'dateTo': 'May 18th'}}, error: 'dialogue_acts'\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "Parsed object is not a dict: None\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "Parsed object is not a dict: None\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "Parsed object is not a dict: None\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "Parsed object is not a dict: None\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "Parsed object is not a dict: None\n",
      "108\n",
      "109\n",
      "Parsed object is not a dict: None\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "Parsed object is not a dict: None\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "Didn't find match: {'system_response': \"Welcome, Lord Fluffington! Although we strive to please all our guests, I'm afraid your... unique requirements may pose some challenges. It's best if I check our availability for such an...\"\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "Parsed object is not a dict: None\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "Parsed object is not a dict: None\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "Parsed object is not a dict: None\n",
      "Parsed object is not a dict: None\n",
      "168\n",
      "169\n",
      "170\n",
      "Parsed object is not a dict: None\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "Parsed object is not a dict: None\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "Didn't find match: {'system_response': \"I'm glad I could help. For check-in, our standard time is 3:00 PM. However, if a room becomes available earlier, we may be able to accommodate you. Check-out time is 11:00 AM, but as long as you inform us at least 24 hours in advance, we can usually arrange a late check-out if needed.\"\n",
      "199\n",
      "Parsed object is not a dict: None\n",
      "200\n",
      "Parsed object is not a dict: None\n",
      "201\n",
      "202\n",
      "203\n",
      "Parsed object is not a dict: None\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "IAR exception: generated {'system_response': \"I understand. Thank you for sharing that. I'll go ahead and process the cancellation for your booking <bookingID>. Are you aware of our cancellation policy?\", 'dialogity_acts': {'intent': 'cancel_booking', 'action': 'cancellation'}, 'belief_state': {'bookingID': 'RES9876'}}, error: 'dialogue_acts'\n",
      "AAR exception: generated {'system_response': \"I understand. Thank you for sharing that. I'll go ahead and process the cancellation for your booking <bookingID>. Are you aware of our cancellation policy?\", 'dialogity_acts': {'intent': 'cancel_booking', 'action': 'cancellation'}, 'belief_state': {'bookingID': 'RES9876'}}, error: 'dialogue_acts'\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "Parsed object is not a dict: None\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "Parsed object is not a dict: None\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "Parsed object is not a dict: None\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m global_step\n\u001b[32m      3\u001b[39m global_step = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m trainer_stats = \u001b[43munsloth_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/unsloth/trainer.py:45\u001b[39m, in \u001b[36munsloth_train\u001b[39m\u001b[34m(trainer, *args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munsloth_train\u001b[39m(trainer, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/transformers/trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/accelerate/utils/memory.py:174\u001b[39m, in \u001b[36mfind_executable_batch_size.<locals>.decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo executable batch size found, reached zero.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:398\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/transformers/trainer.py:3103\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3100\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3103\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3104\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unsloth_compiled_cache/UnslothGRPOTrainer.py:2236\u001b[39m, in \u001b[36m_UnslothGRPOTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   2234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2235\u001b[39m     model_name = \u001b[38;5;28mself\u001b[39m.args.hub_model_id.split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2236\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_model_card\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2237\u001b[39m \u001b[38;5;28msuper\u001b[39m()._save_checkpoint(model, trial)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unsloth_compiled_cache/UnslothGRPOTrainer.py:2288\u001b[39m, in \u001b[36m_UnslothGRPOTrainer.create_model_card\u001b[39m\u001b[34m(self, model_name, dataset_name, tags)\u001b[39m\n\u001b[32m   2275\u001b[39m tags.update(\u001b[38;5;28mself\u001b[39m._tag_names)\n\u001b[32m   2277\u001b[39m citation = textwrap.dedent(\n\u001b[32m   2278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\\\u001b[39;00m\n\u001b[32m   2279\u001b[39m \u001b[33;03m    @article{zhihong2024deepseekmath,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2286\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2288\u001b[39m model_card = \u001b[43mgenerate_model_card\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhub_model_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhub_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwandb_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_wandb_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomet_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_comet_experiment_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGRPO\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer_citation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcitation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpaper_title\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpaper_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2402.03300\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2302\u001b[39m model_card.save(os.path.join(\u001b[38;5;28mself\u001b[39m.args.output_dir, \u001b[33m\"\u001b[39m\u001b[33mREADME.md\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/trl/trainer/utils.py:1615\u001b[39m, in \u001b[36mgenerate_model_card\u001b[39m\u001b[34m(base_model, model_name, hub_model_id, dataset_name, tags, wandb_url, trainer_name, trainer_citation, paper_title, paper_id, comet_url)\u001b[39m\n\u001b[32m   1562\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1563\u001b[39m \u001b[33;03mGenerate a `ModelCard` from a template.\u001b[39;00m\n\u001b[32m   1564\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1591\u001b[39m \u001b[33;03m        A ModelCard object.\u001b[39;00m\n\u001b[32m   1592\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1593\u001b[39m card_data = ModelCardData(\n\u001b[32m   1594\u001b[39m     base_model=base_model,\n\u001b[32m   1595\u001b[39m     datasets=dataset_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1599\u001b[39m     tags=[\u001b[33m\"\u001b[39m\u001b[33mgenerated_from_trainer\u001b[39m\u001b[33m\"\u001b[39m, *tags],\n\u001b[32m   1600\u001b[39m )\n\u001b[32m   1601\u001b[39m card = ModelCard.from_template(\n\u001b[32m   1602\u001b[39m     card_data,\n\u001b[32m   1603\u001b[39m     template_path=\u001b[38;5;28mstr\u001b[39m(pkg_resources.files(\u001b[33m\"\u001b[39m\u001b[33mtrl\u001b[39m\u001b[33m\"\u001b[39m).joinpath(\u001b[33m\"\u001b[39m\u001b[33mtemplates/lm_model_card.md\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m   1604\u001b[39m     base_model=base_model,\n\u001b[32m   1605\u001b[39m     model_name=model_name,\n\u001b[32m   1606\u001b[39m     hub_model_id=hub_model_id,\n\u001b[32m   1607\u001b[39m     dataset_name=dataset_name,\n\u001b[32m   1608\u001b[39m     wandb_url=wandb_url,\n\u001b[32m   1609\u001b[39m     comet_url=comet_url,\n\u001b[32m   1610\u001b[39m     trainer_name=trainer_name,\n\u001b[32m   1611\u001b[39m     trainer_citation=trainer_citation,\n\u001b[32m   1612\u001b[39m     paper_title=paper_title,\n\u001b[32m   1613\u001b[39m     paper_id=paper_id,\n\u001b[32m   1614\u001b[39m     trl_version=version(\u001b[33m\"\u001b[39m\u001b[33mtrl\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m-> \u001b[39m\u001b[32m1615\u001b[39m     transformers_version=\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransformers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m   1616\u001b[39m     pytorch_version=version(\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1617\u001b[39m     datasets_version=version(\u001b[33m\"\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1618\u001b[39m     tokenizers_version=version(\u001b[33m\"\u001b[39m\u001b[33mtokenizers\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1619\u001b[39m )\n\u001b[32m   1620\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m card\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/importlib/metadata/__init__.py:1009\u001b[39m, in \u001b[36mversion\u001b[39m\u001b[34m(distribution_name)\u001b[39m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mversion\u001b[39m(distribution_name):\n\u001b[32m   1003\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[32m   1004\u001b[39m \n\u001b[32m   1005\u001b[39m \u001b[33;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[32m   1006\u001b[39m \u001b[33;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[32m   1007\u001b[39m \u001b[33;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[32m   1008\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/importlib_metadata/__init__.py:557\u001b[39m, in \u001b[36mDistribution.version\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mversion\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    556\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the 'Version' metadata for the distribution package.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmd_none\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mVersion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "global global_step\n",
    "global_step = 0\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "865aa086-5f85-4bca-950f-274e0151508c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T16:07:35.331631Z",
     "iopub.status.busy": "2025-08-31T16:07:35.331231Z",
     "iopub.status.idle": "2025-08-31T16:07:36.451867Z",
     "shell.execute_reply": "2025-08-31T16:07:36.451132Z",
     "shell.execute_reply.started": "2025-08-31T16:07:35.331611Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not determine original model ID from None. If using a local model, ensure the path exists and contains safetensors files.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/unsloth_zoo/saving_utils.py:684\u001b[39m, in \u001b[36mmerge_and_overwrite_lora\u001b[39m\u001b[34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, save_method, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m     file_list = \u001b[43mHfFileSystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetail\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:368\u001b[39m, in \u001b[36mHfFileSystem.ls\u001b[39m\u001b[34m(self, path, detail, refresh, revision, **kwargs)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[33;03mList the contents of a directory.\u001b[39;00m\n\u001b[32m    344\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    366\u001b[39m \u001b[33;03m    dictionaries (if detail=True).\u001b[39;00m\n\u001b[32m    367\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m resolved_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresolve_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    369\u001b[39m path = resolved_path.unresolve()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:229\u001b[39m, in \u001b[36mHfFileSystem.resolve_path\u001b[39m\u001b[34m(self, path, revision)\u001b[39m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m repo_and_revision_exist:\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAccess to repositories lists is not implemented.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m revision = revision \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m constants.DEFAULT_REVISION\n",
      "\u001b[31mNotImplementedError\u001b[39m: Access to repositories lists is not implemented.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained_merged\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_grpo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-merged\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmerged_16bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/unsloth/save.py:2378\u001b[39m, in \u001b[36munsloth_generic_save_pretrained_merged\u001b[39m\u001b[34m(self, save_directory, tokenizer, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   2376\u001b[39m arguments[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m\n\u001b[32m   2377\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2378\u001b[39m \u001b[43munsloth_generic_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m   2380\u001b[39m     gc.collect()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/unsloth/save.py:2324\u001b[39m, in \u001b[36munsloth_generic_save\u001b[39m\u001b[34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   2321\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m save_method == \u001b[33m\"\u001b[39m\u001b[33mmerged_4bit_forced\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2322\u001b[39m     save_method = \u001b[33m\"\u001b[39m\u001b[33mmerged_4bit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2324\u001b[39m \u001b[43mmerge_and_overwrite_lora\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m              \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2335\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2336\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2337\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/dusky5/lib/python3.11/site-packages/unsloth_zoo/saving_utils.py:689\u001b[39m, in \u001b[36mmerge_and_overwrite_lora\u001b[39m\u001b[34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, save_method, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[39m\n\u001b[32m    687\u001b[39m     model_name = original_model_id\n\u001b[32m    688\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m original_model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not determine original model ID from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    690\u001b[39m                         \u001b[33m\"\u001b[39m\u001b[33mIf using a local model, ensure the path exists and contains safetensors files.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    691\u001b[39m     file_list = HfFileSystem(token = token).ls(model_name, detail = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# Process HF file listing\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Could not determine original model ID from None. If using a local model, ensure the path exists and contains safetensors files."
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(experiment_name + \"_grpo\"+ '-merged', tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea74455b-9092-4c9c-b08f-812c48965c05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T22:08:52.339318Z",
     "iopub.status.busy": "2025-08-30T22:08:52.338733Z",
     "iopub.status.idle": "2025-08-30T22:08:52.381353Z",
     "shell.execute_reply": "2025-08-30T22:08:52.380669Z",
     "shell.execute_reply.started": "2025-08-30T22:08:52.339301Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprompt\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "808b864c-c57c-4690-8bae-f70b0f4d8ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T20:59:29.411993Z",
     "iopub.status.busy": "2025-08-20T20:59:29.411631Z",
     "iopub.status.idle": "2025-08-20T20:59:33.654802Z",
     "shell.execute_reply": "2025-08-20T20:59:33.654249Z",
     "shell.execute_reply.started": "2025-08-20T20:59:29.411978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dialogue State: {\\'system_response\\': \"I apologize, but I can only help you with hotel queries. For questions about the meaning of life, you might want to consult a philosopher or a spiritual leader. Is there anything about our hotel services or facilities that I can assist you with?\", \\'dialogue_acts\\': {\\'intent\\': \\'chit_chat\\', \\'action\\': \\'\\'}, \\'belief_state\\': {}}<|end_of_text|>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 250, use_cache = True)\n",
    "extract_llm_response(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba815bddc06d696a",
   "metadata": {},
   "source": [
    "- OpenAIâ€™s GPT Models / Large LLM models\n",
    "- SOTA Approaches - Soloist, SimpleTOD, SimpleTOD, ZS-TOD, AutoTOD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dusky5",
   "language": "python",
   "name": "dusky5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
